{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d13187-d80e-482c-968b-a940a37a0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch.multiprocessing as mp\n",
    "from openpyxl.cell.cell import MergedCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e5c034-70a8-401a-92c9-ea21ab6821aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "recommender_name = \"MLP\" ### Can be MLP, VAE, NCF\n",
    "\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(\"/storage/mikhail/PI4Rec\", DP_DIR)\n",
    "checkpoints_path = Path(export_dir.parent, \"checkpoints\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a416c-493b-402e-b286-c771499cede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7743e4a1-35a0-4433-bb93-7864998f0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\",\n",
    "    \"NCF\": \"single\"}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362}\n",
    "\n",
    "\n",
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),#MLP_ML1M_0.0026_512_14_4.pt\n",
    "    (\"ML1M\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_5e-05_64_16.pt\"),\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    (\"Yahoo\",\"NCF\"):Path(checkpoints_path, \"NCF_Yahoo_0.001_64_21_0.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_12_18_0.0001_256.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    (\"Pinterest\",\"NCF\"):Path(checkpoints_path, \"NCF2_Pinterest_9e-05_32_9_10.pt\"),}\n",
    "\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"NCF\"): 64,\n",
    "}\n",
    "\n",
    "\n",
    "LXR_checkpoint_dict = {\n",
    "    (\"ML1M\",\"VAE\"): ('LXR_ML1M_VAE_26_38_128_3.185652725834087_1.420642300151426.pt',128),\n",
    "    (\"ML1M\",\"MLP\"): ('LXR_ML1M_MLP_19_3_128_13.109692424872248_7.829643365925428.pt',128),\n",
    "    (\"Yahoo\",\"VAE\"): ('LXR_Yahoo_VAE_neg-1.5pos_combined_19_26_128_18.958765029913238_4.92235962483309.pt',128),\n",
    "    (\"Yahoo\",\"MLP\"):('LXR_Yahoo_MLP_neg-pos_combined_last_29_37_128_12.40692505393434_0.19367009952856118.pt',128),\n",
    "    (\"Pinterest\",\"VAE\"): ('LXR_Pinterest_VAE_0_18_64_3.669673618522336_1.7221734058804223.pt',64),\n",
    "    (\"Pinterest\",\"MLP\"):('LXR_Pinterest_MLP_0_5_16_10.059416809308486_0.705778173474644.pt',16),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3c2cf7-0df5-4b52-b94f-113eae5e466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] \n",
    "\n",
    "hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "recommender_path = recommender_path_dict[(data_name,recommender_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1688d9-61e1-4d26-bb30-9d3a6aefe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9342008-cf3d-4057-83fc-7c8a9a239e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = static_test_data.iloc[:,:-2].to_numpy()\n",
    "with open(Path(files_path, f'jaccard_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    jaccard_dict = pickle.load(f) \n",
    "with open(Path(files_path, f'cosine_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    cosine_dict = pickle.load(f) \n",
    "with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f) \n",
    "with open(Path(files_path, f'item_to_cluster_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f) \n",
    "with open(Path(files_path, f'shap_values_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "    shap_values= pickle.load(f) \n",
    "for i in range(num_items):\n",
    "    for j in range(i, num_items):\n",
    "        jaccard_dict[(j,i)]= jaccard_dict[(i,j)]\n",
    "        cosine_dict[(j,i)]= cosine_dict[(i,j)]\n",
    "        pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value\n",
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "           'num_features': num_items, \n",
    "            'demographic':False,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f822bcf-fe8c-47c9-9952-d190dd5d72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/mikhail/PI4Rec/code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "#os.chdir('/storage/mikhail/PI4Rec/code')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79fda75b-849d-4a71-81b1-e4ed749e3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../baselines') \n",
    "from ipynb.fs.defs.help_functions import recommender_run\n",
    "from ipynb.fs.defs.lime import *\n",
    "from ipynb.fs.defs.lime import *\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import *\n",
    "lime = LimeBase(distance_to_proximity)\n",
    "\n",
    "\n",
    "\n",
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *\n",
    "\n",
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "\n",
    "VAE_config= {\n",
    "\"enc_dims\": [512,128],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000}\n",
    "\n",
    "\n",
    "Pinterest_VAE_config= {\n",
    "\"enc_dims\": [256,64],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71481488-7a02-4491-8fe0-44bff4ff114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name == 'MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name == 'VAE':  # This was incorrectly checking for 'MLP' twice\n",
    "        if data_name == \"Pinterest\":\n",
    "            recommender = VAE(Pinterest_VAE_config, **kw_dict)\n",
    "        else:\n",
    "            recommender = VAE(VAE_config, **kw_dict)\n",
    "    elif recommender_name == 'NCF':\n",
    "        MLP_temp = MLP_model(hidden_size=hidden_dim, num_layers=3, **kw_dict)\n",
    "        GMF_temp = GMF_model(hidden_size=hidden_dim, **kw_dict)\n",
    "        recommender = NCF(factor_num=hidden_dim, num_layers=3, dropout=0.5, model='NeuMF-pre', GMF_model=GMF_temp, MLP_model=MLP_temp, **kw_dict)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown recommender name: {recommender_name}\")\n",
    "\n",
    "    # Check if the model's state_dict matches the architecture\n",
    "    recommender_checkpoint = torch.load(recommender_path, map_location=device)\n",
    "    recommender.load_state_dict(recommender_checkpoint, strict=False)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede02658-6ced-4dd1-b2d0-c94aacfa2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pop_mask(x, item_id):\n",
    "    user_hist = torch.Tensor(x).to(device) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    \n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            item_pop_dict[i]=pop_array[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46eddb52-a66a-4fd4-929a-5622c0264b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_mask(x, item_id, user_based_Jaccard_sim):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in user_based_Jaccard_sim:\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0            \n",
    "\n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c57942e-ce2c-47ba-900c-77cd7906e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, item_cosine):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in item_cosine:\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)]\n",
    "            else:\n",
    "                item_cosine_dict[i]=0\n",
    "\n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96c8a6f-de19-4e0d-8f03-f49c05f06120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(nn.Module):\n",
    "    def __init__(self, user_size, item_size, hidden_size):\n",
    "        super(Explainer, self).__init__()\n",
    "        \n",
    "        self.users_fc = nn.Linear(in_features = user_size, out_features=hidden_size).to(device)\n",
    "        self.items_fc = nn.Linear(in_features = item_size, out_features=hidden_size).to(device)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size).to(device),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size, out_features=user_size).to(device),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_output = self.users_fc(user_tensor.float())\n",
    "        item_output = self.items_fc(item_tensor.float())\n",
    "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
    "        expl_scores = self.bottleneck(combined_output).to(device)\n",
    "        return expl_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ecf88b-e353-436a-bf6e-14b70c7e4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lxr_mask(x, item_tensor, explainer):\n",
    "    user_hist = x\n",
    "    expl_scores = explainer(user_hist, item_tensor)\n",
    "    \n",
    "    # Debug: Print the explainer scores and user history\n",
    "    print(f\"Explainer scores: {expl_scores[:5]}\")\n",
    "    print(f\"User history before masking: {user_hist[:5]}\")\n",
    "    \n",
    "    x_masked = user_hist * expl_scores\n",
    "    \n",
    "    # Debug: Print masked user history\n",
    "    print(f\"User history after masking: {x_masked[:5]}\")\n",
    "    \n",
    "    item_sim_dict = {}\n",
    "    for i, j in enumerate(x_masked != 0):\n",
    "        if j:\n",
    "            item_sim_dict[i] = x_masked[i]\n",
    "    \n",
    "    return item_sim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23814019-635a-47a3-8024-91422268e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_explainer(fine_tuning=False, lambda_pos=None, lambda_neg=None, alpha=None):\n",
    "    lxr_path, lxr_dim = LXR_checkpoint_dict[(data_name, recommender_name)]\n",
    "    explainer = Explainer(num_items, num_items, lxr_dim)\n",
    "    lxr_checkpoint = torch.load(Path(checkpoints_path, lxr_path))\n",
    "    explainer.load_state_dict(lxr_checkpoint)\n",
    "    explainer.eval()\n",
    "    for param in explainer.parameters():\n",
    "        param.requires_grad = False\n",
    "    return explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3884e0-32fc-4aea-a6d6-921d5cd862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lime_mask(x, item_id, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, recommender, num_samples=10, method = 'POS', **kw_dict):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lime_args(user_hist, item_id, recommender, all_items_tensor, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id, **kw_dict)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d96b0b-c479-4f15-ab5a-ca6e0c8df383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lire_mask(x, item_id, num_of_perturbations, kernel_func, feature_selection, recommender, proba=0.1, method = 'POS', **kw_dict):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lire_args(user_hist, item_id, recommender, all_items_tensor, train_array, num_of_perturbations = num_of_perturbations, seed = item_id, proba=0.1, **kw_dict)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_of_perturbations, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_of_perturbations, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c69a9ca0-b6cd-4171-a63e-5e516af77ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fia_mask(user_tensor, item_tensor, item_id, recommender):\n",
    "    y_pred = recommender_run(user_tensor, recommender, item_tensor, item_id, **kw_dict).to(device)\n",
    "    items_fia = {}\n",
    "    user_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        if(user_hist[i] == 1):\n",
    "            user_hist[i] = 0\n",
    "            user_tensor = torch.FloatTensor(user_hist).to(device)\n",
    "            y_pred_without_item = recommender_run(user_tensor, recommender, item_tensor, item_id, 'single', **kw_dict).to(device)\n",
    "            infl_score = y_pred - y_pred_without_item\n",
    "            items_fia[i] = infl_score\n",
    "            user_hist[i] = 1\n",
    "\n",
    "    return items_fia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6fe48a9-c488-488d-ba29-091c3d6b92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_tensor, user_id, model, shap_values, item_to_cluster):\n",
    "    item_shap = {}\n",
    "    shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,1:]\n",
    "    user_vector = user_tensor.cpu().detach().numpy().astype(int)\n",
    "\n",
    "    for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "        items_cluster = item_to_cluster[i]\n",
    "        item_shap[i] = shapley_values.T[int(items_cluster)][0]\n",
    "\n",
    "    return item_shap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ffa351-22c2-426b-a5be-f87efdffa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, top_k):\n",
    "   \n",
    "    items_accent = defaultdict(float)\n",
    "    factor = top_k - 1\n",
    "    user_accent_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "\n",
    "    #Get topk items\n",
    "    sorted_indices = list(get_top_k(user_tensor, user_tensor, recommender_model, **kw_dict).keys())\n",
    "    \n",
    "    if top_k == 1:\n",
    "        # When k=1, return the index of the first maximum value\n",
    "        top_k_indices = [sorted_indices[0]]\n",
    "    else:\n",
    "        top_k_indices = sorted_indices[:top_k]\n",
    "   \n",
    "\n",
    "    for iteration, item_k_id in enumerate(top_k_indices):\n",
    "\n",
    "        # Set topk items to 0 in the user's history\n",
    "        user_accent_hist[item_k_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_accent_hist).to(device)\n",
    "       \n",
    "        item_vector = items_array[item_k_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "              \n",
    "        # Check influence of the items in the history on this specific item in topk\n",
    "        fia_dict = find_fia_mask(user_tensor, item_tensor, item_k_id, recommender_model)\n",
    "         \n",
    "        # Sum up all differences between influence on top1 and other topk values\n",
    "        if not iteration:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] *= factor\n",
    "        else:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] -= fia_dict[key]\n",
    "       \n",
    "    for key in items_accent.keys():\n",
    "        items_accent[key] *= -1    \n",
    "\n",
    "    return items_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e85c73d-f81c-42c3-874f-99fd49065e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender_model, user_id = None, mask_type = None):\n",
    "    '''\n",
    "    This function invokes various explanation functions\n",
    "    and returns a dictionary of explanations, sorted by their scores.\n",
    "    '''\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    if mask_type == 'lime':\n",
    "        POS_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity, 'highest_weights', recommender_model, num_samples=user_hist_size, **kw_dict)\n",
    "        NEG_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity, 'highest_weights', recommender_model, num_samples=user_hist_size, method='NEG', **kw_dict)\n",
    "    elif mask_type == 'lire':\n",
    "        POS_sim_items = find_lire_mask(user_vector, item_id, 200, distance_to_proximity, 'highest_weights', recommender_model, proba=0.1, **kw_dict)\n",
    "        NEG_sim_items = find_lire_mask(user_vector, item_id, 200, distance_to_proximity, 'highest_weights', recommender_model, proba=0.1, method='NEG', **kw_dict)\n",
    "    else:\n",
    "        if mask_type == 'cosine':\n",
    "            sim_items = find_cosine_mask(user_tensor, item_id, cosine_dict)\n",
    "        elif mask_type == 'shap':\n",
    "            sim_items = find_shapley_mask(user_tensor, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "        elif mask_type == 'accent':\n",
    "            sim_items = find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, 5)\n",
    "        elif mask_type == 'lxr':\n",
    "            explainer = load_explainer(True)\n",
    "            sim_items = find_lxr_mask(user_tensor, item_tensor, explainer)\n",
    "\n",
    "        POS_sim_items = list(sorted(sim_items.items(), key=lambda item: item[1], reverse=True))[0:user_hist_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f221f8-c75a-477c-856e-001e8e2bddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, user_tensor, item_id, item_tensor, recommender_model, expl_dict, **kw_dict):\n",
    "    \"\"\"\n",
    "    Calculate metrics for a single user with 5 steps of item masking\n",
    "    Now explicitly includes POS@20 and NEG@20 metrics\n",
    "    \"\"\"\n",
    "    POS_masked = user_tensor.clone()\n",
    "    NEG_masked = user_tensor.clone()\n",
    "    POS_masked[item_id] = 0\n",
    "    NEG_masked[item_id] = 0\n",
    "    \n",
    "    # Use 5 steps\n",
    "    num_steps = 5\n",
    "    bins = range(1, num_steps + 1)  # [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # Initialize metric arrays with 5 elements each\n",
    "    POS_at_5 = [0] * num_steps\n",
    "    POS_at_10 = [0] * num_steps\n",
    "    POS_at_20 = [0] * num_steps  # Explicitly initialize POS@20\n",
    "    \n",
    "    NEG_at_5 = [0] * num_steps\n",
    "    NEG_at_10 = [0] * num_steps\n",
    "    NEG_at_20 = [0] * num_steps  # Explicitly initialize NEG@20\n",
    "    \n",
    "    DEL = [0] * num_steps\n",
    "    INS = [0] * num_steps\n",
    "    NDCG = [0] * num_steps\n",
    "    \n",
    "    # Get sorted items by importance\n",
    "    POS_sim_items = expl_dict\n",
    "    NEG_sim_items = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1], reverse=False))\n",
    "    \n",
    "    # For each step (1 to 5 items)\n",
    "    for i, num_items_to_mask in enumerate(bins):\n",
    "        # Create masks for positive and negative cases\n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=kw_dict['device'])\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=kw_dict['device'])\n",
    "        \n",
    "        # Apply POS masking\n",
    "        for j in POS_sim_items[:num_items_to_mask]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor * (1 - POS_masked)\n",
    "        \n",
    "        # Apply NEG masking\n",
    "        for j in NEG_sim_items[:num_items_to_mask]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor * (1 - NEG_masked)\n",
    "        \n",
    "        # Get rankings for both POS and NEG\n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor, recommender_model, **kw_dict)\n",
    "        NEG_ranked_list = get_top_k(NEG_masked, user_tensor, recommender_model, **kw_dict)\n",
    "        \n",
    "        # Calculate POS ranking\n",
    "        if item_id in list(POS_ranked_list.keys()):\n",
    "            POS_index = list(POS_ranked_list.keys()).index(item_id) + 1\n",
    "        else:\n",
    "            POS_index = kw_dict['num_items']\n",
    "            \n",
    "        # Calculate NEG ranking\n",
    "        if item_id in list(NEG_ranked_list.keys()):\n",
    "            NEG_index = list(NEG_ranked_list.keys()).index(item_id) + 1\n",
    "        else:\n",
    "            NEG_index = kw_dict['num_items']\n",
    "        \n",
    "        # Calculate ALL metrics including P@20\n",
    "        POS_at_5[i] = 1 if POS_index <= 5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <= 10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <= 20 else 0  # Explicitly calculate POS@20\n",
    "        \n",
    "        NEG_at_5[i] = 1 if NEG_index <= 5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <= 10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <= 20 else 0  # Explicitly calculate NEG@20\n",
    "        \n",
    "        # Calculate other metrics\n",
    "        DEL[i] = float(recommender_run(POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "        INS[i] = float(recommender_run(user_tensor - POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "        NDCG[i] = get_ndcg(list(POS_ranked_list.keys()), item_id, **kw_dict)\n",
    "    \n",
    "    # Return all metrics including P@20\n",
    "    res = [DEL, INS, NDCG, \n",
    "           POS_at_5, POS_at_10, POS_at_20,  # Include POS@20\n",
    "           NEG_at_5, NEG_at_10, NEG_at_20]  # Include NEG@20\n",
    "    return [np.array(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e54714f-3733-49a6-a2c1-eef605495be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsBaselines:\n",
    "    def __init__(self, data_name, recommender_name):\n",
    "        self.data_name = data_name\n",
    "        self.recommender_name = recommender_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_data_and_recommender()\n",
    "\n",
    "    def setup_data_and_recommender(self):\n",
    "        # Set up all necessary data and variables\n",
    "        DP_DIR = Path(\"processed_data\", self.data_name)\n",
    "        self.files_path = Path(export_dir.parent, DP_DIR)\n",
    "        self.num_users = num_users_dict[self.data_name]\n",
    "        self.num_items = num_items_dict[self.data_name]\n",
    "        \n",
    "        self.test_data = pd.read_csv(Path(self.files_path, f'test_data_{self.data_name}.csv'), index_col=0)\n",
    "        self.test_array = self.test_data.to_numpy()\n",
    "        self.items_array = np.eye(self.num_items)\n",
    "        \n",
    "        with open(Path(self.files_path, f'pop_dict_{self.data_name}.pkl'), 'rb') as f:\n",
    "            self.pop_dict = pickle.load(f)\n",
    "        \n",
    "        # Load other necessary data (jaccard_dict, cosine_dict, item_to_cluster, shap_values)\n",
    "        \n",
    "        self.kw_dict = {\n",
    "            'device': self.device,\n",
    "            'num_items': self.num_items,\n",
    "            'num_features': self.num_items,\n",
    "            'demographic': False,\n",
    "            'pop_array': np.array([self.pop_dict.get(i, 0) for i in range(self.num_items)]),\n",
    "            'all_items_tensor': torch.eye(self.num_items).to(self.device),\n",
    "            'static_test_data': self.test_data,\n",
    "            'items_array': self.items_array,\n",
    "            'output_type': output_type_dict[self.recommender_name],\n",
    "            'recommender_name': self.recommender_name,\n",
    "            'files_path': self.files_path\n",
    "        }\n",
    "        \n",
    "        self.recommender = self.load_recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80af0804-97c7-40ce-b7b3-e716d98d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user(user_index, test_array, test_data, recommender, kw_dict):\n",
    "    try:\n",
    "        user_vector = test_array[user_index]\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(kw_dict['device'])\n",
    "        user_id = int(test_data.index[user_index])\n",
    "\n",
    "        item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "        item_vector = kw_dict['items_array'][item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(kw_dict['device'])\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor[item_id] = 0\n",
    "\n",
    "        results = {}\n",
    "        for method in ['pop', 'jaccard', 'cosine', 'lime', 'lxr', 'accent', 'shap']:\n",
    "            results[method] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, kw_dict['num_items'], recommender, mask_type=method, user_id=user_id if method == 'shap' else None)\n",
    "\n",
    "        return user_id, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user {user_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5014335c-4a98-4639-b404-337fe9864cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_expl_type(expl_name):\n",
    "    print(f' ============ Start explaining {data_name} {recommender_name} by {expl_name} ============')\n",
    "    \n",
    "    # Load the appropriate explanation dictionary\n",
    "    if expl_name == 'PI_base':\n",
    "        with open(Path(files_path, f'{recommender_name}_PI_base_expl_dict.pkl'), 'rb') as handle:\n",
    "            expl_dict = pickle.load(handle)\n",
    "    else:\n",
    "        with open(Path(files_path,f'{recommender_name}_{expl_name}_expl_dict.pkl'), 'rb') as handle:\n",
    "            expl_dict = pickle.load(handle)\n",
    "    \n",
    "    recommender.eval()\n",
    "    \n",
    "    # Initialize arrays for metrics with 5 steps\n",
    "    num_steps = 5\n",
    "    users_DEL = np.zeros(num_steps)\n",
    "    users_INS = np.zeros(num_steps)\n",
    "    NDCG = np.zeros(num_steps)\n",
    "    \n",
    "    POS_at_5 = np.zeros(num_steps)\n",
    "    POS_at_10 = np.zeros(num_steps)\n",
    "    POS_at_20 = np.zeros(num_steps)\n",
    "    \n",
    "    NEG_at_5 = np.zeros(num_steps)\n",
    "    NEG_at_10 = np.zeros(num_steps)\n",
    "    NEG_at_20 = np.zeros(num_steps)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(test_array.shape[0])):\n",
    "            user_vector = test_array[i]\n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            user_id = int(test_data.index[i])\n",
    "\n",
    "            item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "            item_vector = items_array[item_id]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "            user_vector[item_id] = 0\n",
    "            user_tensor[item_id] = 0\n",
    "\n",
    "            user_expl = expl_dict[user_id]\n",
    "\n",
    "            res = single_user_metrics(user_vector, user_tensor, item_id, item_tensor, recommender, user_expl, **kw_dict)\n",
    "            users_DEL += res[0]\n",
    "            users_INS += res[1]\n",
    "            NDCG += res[2]\n",
    "            POS_at_5 += res[3]\n",
    "            POS_at_10 += res[4]\n",
    "            POS_at_20 += res[5]\n",
    "            NEG_at_5 += res[6]\n",
    "            NEG_at_10 += res[7]\n",
    "            NEG_at_20 += res[8]\n",
    "\n",
    "    a = test_array.shape[0]\n",
    "\n",
    "    print(f'users_DEL_{expl_name}: ', np.mean(users_DEL)/a)\n",
    "    print(f'users_INS_{expl_name}: ', np.mean(users_INS)/a)\n",
    "    print(f'NDCG_{expl_name}: ', np.mean(NDCG)/a)\n",
    "    print(f'POS_at_5_{expl_name}: ', np.mean(POS_at_5)/a)\n",
    "    print(f'POS_at_10_{expl_name}: ', np.mean(POS_at_10)/a)\n",
    "    print(f'POS_at_20_{expl_name}: ', np.mean(POS_at_20)/a)\n",
    "    print(f'NEG_at_5_{expl_name}: ', np.mean(NEG_at_5)/a)\n",
    "    print(f'NEG_at_10_{expl_name}: ', np.mean(NEG_at_10)/a)\n",
    "    print(f'NEG_at_20_{expl_name}: ', np.mean(NEG_at_20)/a)\n",
    "\n",
    "    return {\n",
    "        'DEL': users_DEL/a,\n",
    "        'INS': users_INS/a,\n",
    "        'NDCG': NDCG/a,\n",
    "        'POS_at_5': POS_at_5/a,\n",
    "        'POS_at_10': POS_at_10/a,\n",
    "        'POS_at_20': POS_at_20/a,\n",
    "        'NEG_at_5': NEG_at_5/a,\n",
    "        'NEG_at_10': NEG_at_10/a,\n",
    "        'NEG_at_20': NEG_at_20/a\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04fcdd0a-259c-411b-bc56-ac0c6e10dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_baselines(data_name, recommender_name):\n",
    "    global num_users, num_items, device, kw_dict, recommender, test_array, test_data, items_array, jaccard_dict, cosine_dict, pop_dict, item_to_cluster, shap_values\n",
    "\n",
    "    # Update global variables for the current dataset and recommender\n",
    "    num_users = num_users_dict[data_name]\n",
    "    num_items = num_items_dict[data_name]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load dataset-specific files\n",
    "    DP_DIR = Path(\"processed_data\", data_name)\n",
    "    files_path = Path(export_dir.parent, DP_DIR)\n",
    "    test_data = pd.read_csv(Path(files_path, f'test_data_{data_name}.csv'), index_col=0)\n",
    "    test_array = test_data.to_numpy()\n",
    "    items_array = np.eye(num_items)\n",
    "\n",
    "    with open(Path(files_path, f'jaccard_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "        jaccard_dict = pickle.load(f)\n",
    "    with open(Path(files_path, f'cosine_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "        cosine_dict = pickle.load(f)\n",
    "    with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "        pop_dict = pickle.load(f)\n",
    "    with open(Path(files_path, f'item_to_cluster_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "        item_to_cluster = pickle.load(f)\n",
    "    with open(Path(files_path, f'shap_values_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "        shap_values = pickle.load(f)\n",
    "\n",
    "    # Update kw_dict\n",
    "    kw_dict = {\n",
    "        'device': device,\n",
    "        'num_items': num_items,\n",
    "        'num_features': num_items,\n",
    "        'demographic': False,\n",
    "        'pop_array': np.array([pop_dict.get(i, 0) for i in range(num_items)]),\n",
    "        'all_items_tensor': torch.eye(num_items).to(device),\n",
    "        'static_test_data': test_data,\n",
    "        'items_array': items_array,\n",
    "        'output_type': output_type_dict[recommender_name],\n",
    "        'recommender_name': recommender_name,\n",
    "        'files_path': files_path\n",
    "    }\n",
    "\n",
    "    # Load recommender\n",
    "    recommender = load_recommender()\n",
    "\n",
    "    # Run all baselines\n",
    "    baselines = [ 'jaccard', 'cosine', 'lime', 'lxr', 'accent', 'shap']\n",
    "    results = {}\n",
    "\n",
    "    for baseline in baselines:\n",
    "        print(f\"Running {baseline} baseline for {data_name} {recommender_name}\")\n",
    "        results[baseline] = eval_one_expl_type(baseline)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee87b2df-9d64-48c0-b233-c9553760ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics(results, data_name, recommender_name):\n",
    "    # Mapping between metric keys and their display names (title, y-label, indicator)\n",
    "    metrics_mapping = {\n",
    "        'DEL':      ('AUC DEL-P@K', 'DEL@Kₑ', 'Lower is better'),\n",
    "        'INS':      ('AUC INS-P@K', 'INS@Kₑ', 'Higher is better'),\n",
    "        'NDCG':     ('AUC NDCG-P',  'CNDCG@Kₑ', 'Lower is better'),\n",
    "        'POS_at_5': ('AUC POS-P@5', 'POS@20Kₑ', 'Lower is better'),\n",
    "        'POS_at_10':('AUC POS-P@10','POS@20Kₑ', 'Lower is better'),\n",
    "        'POS_at_20':('AUC POS-P@20','POS@20Kₑ', 'Lower is better'),\n",
    "        'NEG_at_5': ('AUC NEG-P@5', 'NEG@20Kₑ', 'Higher is better'),\n",
    "        'NEG_at_10':('AUC NEG-P@10','NEG@20Kₑ', 'Higher is better'),\n",
    "        'NEG_at_20':('AUC NEG-P@20','NEG@20Kₑ', 'Higher is better')\n",
    "    }\n",
    "    \n",
    "    # Style lists\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'x']\n",
    "    linestyles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 2))]\n",
    "    \n",
    "    # Create plots directory\n",
    "    os.makedirs('plots_discrete', exist_ok=True)\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric, (title_name, y_label, indicator) in metrics_mapping.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for i, (baseline, baseline_metrics) in enumerate(results.items()):\n",
    "            if metric not in baseline_metrics:\n",
    "                print(f\"Warning: {metric} not found in {baseline} metrics\")\n",
    "                continue\n",
    "            \n",
    "            values = baseline_metrics[metric][:5]  # Take only first 5 values\n",
    "            x = range(1, len(values) + 1)  # Numbers of masked items (1, 2, 3, 4, 5)\n",
    "            \n",
    "            plt.plot(\n",
    "                x, values,\n",
    "                label=baseline.upper(),\n",
    "                color=colors[i % len(colors)],\n",
    "                linestyle=linestyles[i % len(linestyles)],\n",
    "                marker=markers[i % len(markers)],\n",
    "                markersize=8,\n",
    "                linewidth=2,\n",
    "                markevery=1  # Markers on each value\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(\"Number of Masked Items\", fontsize=30)\n",
    "        plt.ylabel(y_label, fontsize=30)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5)\n",
    "        plt.xticks(range(1, 6), fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.legend(\n",
    "            loc='best', \n",
    "            fontsize=20,\n",
    "            frameon=True,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        safe_display_name = title_name.replace(\" \", \"_\").replace(\"@\", \"at\")\n",
    "        plt.savefig(f'plots_discrete/{safe_display_name}_{data_name}_{recommender_name}d.pdf',\n",
    "                    format='pdf', bbox_inches='tight')\n",
    "        print(f\"Saved plot: {safe_display_name}_{data_name}_{recommender_name}d.pdf\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d41114e-a2f1-45ab-9782-dc2a3dc71235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recommender(data_name, recommender_name):\n",
    "    DP_DIR = Path(\"processed_data\", data_name)\n",
    "    files_path = Path(\"/storage/mikhail/PI4Rec\", DP_DIR)\n",
    "    \n",
    "    num_users = num_users_dict[data_name]\n",
    "    num_items = num_items_dict[data_name]\n",
    "    num_features = num_items_dict[data_name]\n",
    "    \n",
    "    with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "        pop_dict = pickle.load(f)\n",
    "    pop_array = np.zeros(len(pop_dict))\n",
    "    for key, value in pop_dict.items():\n",
    "        pop_array[key] = value\n",
    "\n",
    "    test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "    static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "    \n",
    "    test_array = static_test_data.iloc[:,:-2].to_numpy()\n",
    "    items_array = np.eye(num_items)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    all_items_tensor = torch.Tensor(items_array).to(device)\n",
    "\n",
    "    output_type = output_type_dict[recommender_name]\n",
    "    hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "    recommender_path = recommender_path_dict[(data_name,recommender_name)]\n",
    "\n",
    "    kw_dict = {\n",
    "        'device': device,\n",
    "        'num_items': num_items,\n",
    "        'demographic': False,\n",
    "        'num_features': num_features,\n",
    "        'pop_array': pop_array,\n",
    "        'all_items_tensor': all_items_tensor,\n",
    "        'static_test_data': static_test_data,\n",
    "        'items_array': items_array,\n",
    "        'output_type': output_type,\n",
    "        'recommender_name': recommender_name,\n",
    "        'files_path': files_path\n",
    "    }\n",
    "\n",
    "    recommender = load_recommender()\n",
    "\n",
    "    print(f\"Processing {data_name} dataset with {recommender_name} recommender\")\n",
    "    \n",
    "    results = {}\n",
    "    for expl_name in [ 'jaccard', 'cosine', 'lime', 'lxr', 'accent', 'shap']:\n",
    "        results[expl_name] = eval_one_expl_type(expl_name, data_name, recommender_name, test_array, test_data, items_array, recommender, kw_dict)\n",
    "    \n",
    "   # plot_all_metrics(results, data_name, recommender_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b97d008a-787f-4e42-8999-1ce760cdf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment, Border, Side\n",
    "\n",
    "def save_results_to_excel(results, filename):\n",
    "    wb = Workbook()\n",
    "    \n",
    "    # Create MF recommender sheet\n",
    "    ws_mf = wb.active\n",
    "    ws_mf.title = \"MF Recommender\"\n",
    "    \n",
    "    # Create VAE recommender sheet\n",
    "    ws_vae = wb.create_sheet(title=\"VAE Recommender\")\n",
    "    \n",
    "    for ws, title in [(ws_mf, \"AUC values for explaining an MF recommender.\"), \n",
    "                      (ws_vae, \"AUC values for explaining a VAE recommender.\")]:\n",
    "        \n",
    "        # Add title\n",
    "        ws['A1'] = f\"Table: {title}\"\n",
    "        ws['A1'].font = Font(bold=True)\n",
    "        ws.merge_cells('A1:G1')\n",
    "        \n",
    "        # Add headers\n",
    "        headers = ['Method', 'k=5', 'k=10', 'k=20', 'DEL', 'INS', 'NDCG']\n",
    "        for col, header in enumerate(headers, start=1):\n",
    "            ws.cell(row=3, column=col, value=header).font = Font(bold=True)\n",
    "        \n",
    "        # Add data\n",
    "        for row, (method, values) in enumerate(results.items(), start=4):\n",
    "            ws.cell(row=row, column=1, value=method)\n",
    "            for col, value in enumerate(values, start=2):\n",
    "                ws.cell(row=row, column=col, value=value)\n",
    "    \n",
    "    # Apply some styling\n",
    "    for ws in [ws_mf, ws_vae]:\n",
    "        for row in ws[f'A3:G{ws.max_row}']:\n",
    "            for cell in row:\n",
    "                cell.border = Border(left=Side(style='thin'), \n",
    "                                     right=Side(style='thin'), \n",
    "                                     top=Side(style='thin'), \n",
    "                                     bottom=Side(style='thin'))\n",
    "    \n",
    "    wb.save(filename)\n",
    "\n",
    "def run_and_format_results(data_name, recommender_name):\n",
    "    results = {}\n",
    "    for expl_name in ['jaccard', 'cosine', 'lime', 'shap', 'accent', 'lxr']:\n",
    "        raw_results = eval_one_expl_type(expl_name)\n",
    "        \n",
    "        # Extract POS values\n",
    "        pos_at_5 = raw_results['POS_at_5'][-1]  # Last value represents 100% of items\n",
    "        pos_at_10 = raw_results['POS_at_10'][-1]\n",
    "        pos_at_20 = raw_results['POS_at_20'][-1]\n",
    "        \n",
    "        # Format results as per the desired output\n",
    "        results[expl_name.upper()] = [\n",
    "            pos_at_5,\n",
    "            pos_at_10,\n",
    "            pos_at_20,\n",
    "            raw_results['DEL'][-1],\n",
    "            raw_results['INS'][-1],\n",
    "            raw_results['NDCG'][-1]\n",
    "        ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc066cb-1713-4be6-8bb7-9b5ae8f0707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def create_comparison_visualizations(all_results, save_dir='./'): \n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations comparing methods across datasets and recommenders\n",
    "    \n",
    "    Parameters:\n",
    "    all_results: dict\n",
    "        Format: {(dataset_name, recommender_name): results_dict}\n",
    "        where results_dict contains metrics for each explanation method\n",
    "    save_dir: str\n",
    "        Directory to save the visualization files\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for (dataset, recommender), results in all_results.items():\n",
    "        for method, metrics in results.items():\n",
    "            for metric_name, values in metrics.items():\n",
    "                if isinstance(values, np.ndarray):\n",
    "                    for step, value in enumerate(values, 1):\n",
    "                        plot_data.append({\n",
    "                            'Dataset': dataset,\n",
    "                            'Recommender': recommender,\n",
    "                            'Method': method.upper(),\n",
    "                            'Metric': metric_name,\n",
    "                            'Step': step,\n",
    "                            'Value': value\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # 1. Heatmap of method performance across datasets and recommenders\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics_to_plot = ['DEL', 'INS', 'NDCG']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(1, 3, idx+1)\n",
    "        pivot_data = df[df['Metric'] == metric].groupby(\n",
    "            ['Dataset', 'Recommender', 'Method'])['Value'].mean().unstack()\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "        plt.title(f'{metric} Performance Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/performance_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Method stability analysis (standard deviation across steps)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    stability_data = df.groupby(['Method', 'Metric'])['Value'].std().unstack()\n",
    "    stability_data.plot(kind='bar', width=0.8)\n",
    "    plt.title('Method Stability Analysis (Standard Deviation Across Steps)')\n",
    "    plt.xlabel('Explanation Method')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/method_stability.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Radar chart for method comparison\n",
    "    def create_radar_chart(data, methods, metrics):\n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        for method in methods:\n",
    "            values = [data[(data['Method'] == method) & \n",
    "                          (data['Metric'] == metric)]['Value'].mean() \n",
    "                     for metric in metrics]\n",
    "            values += values[:1]\n",
    "            angles_plot = np.concatenate([angles, [angles[0]]])\n",
    "            ax.plot(angles_plot, values, 'o-', label=method)\n",
    "            ax.fill(angles_plot, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    metrics_for_radar = ['DEL', 'INS', 'NDCG', 'POS_at_5', 'POS_at_10']\n",
    "    for dataset in df['Dataset'].unique():\n",
    "        for recommender in df['Recommender'].unique():\n",
    "            data_subset = df[(df['Dataset'] == dataset) & \n",
    "                           (df['Recommender'] == recommender)]\n",
    "            fig = create_radar_chart(data_subset, \n",
    "                                   df['Method'].unique(), \n",
    "                                   metrics_for_radar)\n",
    "            plt.title(f'{dataset} - {recommender}\\nMethod Comparison')\n",
    "            plt.savefig(f'{save_dir}/radar_{dataset}_{recommender}.png')\n",
    "            plt.close()\n",
    "\n",
    "    # 4. Box plots showing distribution of metrics across steps\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for idx, metric in enumerate(['DEL', 'INS', 'NDCG'], 1):\n",
    "        plt.subplot(1, 3, idx)\n",
    "        sns.boxplot(data=df[df['Metric'] == metric], \n",
    "                   x='Method', y='Value', \n",
    "                   hue='Dataset')\n",
    "        plt.title(f'{metric} Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/metric_distributions.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Performance improvement over steps\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for metric in ['DEL', 'INS', 'NDCG']:\n",
    "        plt.subplot(1, 3, metrics_to_plot.index(metric) + 1)\n",
    "        for method in df['Method'].unique():\n",
    "            data = df[(df['Metric'] == metric) & (df['Method'] == method)]\n",
    "            plt.plot(data.groupby('Step')['Value'].mean(), \n",
    "                    marker='o', \n",
    "                    label=method)\n",
    "        plt.title(f'{metric} Progress Over Steps')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Value')\n",
    "        if metric == 'NDCG':\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/progress_over_steps.png')\n",
    "    plt.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_summary_report(df, save_dir='./'):\n",
    "    \"\"\"\n",
    "    Generates a statistical summary report of the results\n",
    "    \"\"\"\n",
    "    # Calculate aggregate statistics\n",
    "    summary = df.groupby(['Dataset', 'Recommender', 'Method', 'Metric'])['Value'].agg([\n",
    "        'mean', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary.to_csv(f'{save_dir}/summary_statistics.csv')\n",
    "    \n",
    "    # Calculate method rankings\n",
    "    rankings = df.groupby(['Dataset', 'Recommender', 'Method', 'Metric'])['Value'].mean().unstack()\n",
    "    method_ranks = rankings.rank(ascending=False, method='min')\n",
    "    method_ranks.to_csv(f'{save_dir}/method_rankings.csv')\n",
    "    \n",
    "    return summary, method_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e99013e-7482-423b-b981-c32658f279c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(results, data_name, recommender_name):\n",
    "    \"\"\"\n",
    "    Create a comprehensive table of all metrics for each method and masking step\n",
    "    \"\"\"\n",
    "    # Initialize the table structure\n",
    "    table_data = []\n",
    "    metrics = ['DEL', 'INS', 'NDCG', 'POS_at_5', 'POS_at_10', 'POS_at_20']\n",
    "    \n",
    "    for method in results.keys():\n",
    "        for step in range(5):  # 5 steps\n",
    "            row = {\n",
    "                'Method': method.upper(),\n",
    "                'Step': step + 1,  # 1-based indexing\n",
    "                'Dataset': data_name,\n",
    "                'Recommender': recommender_name\n",
    "            }\n",
    "            \n",
    "            # Add all metrics for this method and step\n",
    "            for metric in metrics:\n",
    "                row[metric] = results[method][metric][step]\n",
    "            \n",
    "            table_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f'results_{data_name}_{recommender_name}.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Create and save a formatted Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = f\"{data_name}_{recommender_name}_Results\"\n",
    "    \n",
    "    # Add title (in row 1)\n",
    "    ws['A1'] = f\"Results for {data_name} dataset with {recommender_name} recommender\"\n",
    "    ws.merge_cells('A1:H1')\n",
    "    ws['A1'].font = Font(bold=True)\n",
    "    \n",
    "    # Add headers (in row 3)\n",
    "    headers = ['Method', 'Step', 'DEL', 'INS', 'NDCG', 'POS@5', 'POS@10', 'POS@20']\n",
    "    for col, header in enumerate(headers, 1):\n",
    "        cell = ws.cell(row=3, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "    \n",
    "    # Add data with formatting\n",
    "    current_method = None\n",
    "    row_num = 4\n",
    "    for _, row in df.iterrows():\n",
    "        if current_method != row['Method']:\n",
    "            current_method = row['Method']\n",
    "            row_num += 1  # Add space between methods\n",
    "        \n",
    "        ws.cell(row=row_num, column=1, value=row['Method'])\n",
    "        ws.cell(row=row_num, column=2, value=row['Step'])\n",
    "        ws.cell(row=row_num, column=3, value=float(row['DEL']))\n",
    "        ws.cell(row=row_num, column=4, value=float(row['INS']))\n",
    "        ws.cell(row=row_num, column=5, value=float(row['NDCG']))\n",
    "        ws.cell(row=row_num, column=6, value=float(row['POS_at_5']))\n",
    "        ws.cell(row=row_num, column=7, value=float(row['POS_at_10']))\n",
    "        ws.cell(row=row_num, column=8, value=float(row['POS_at_20']))\n",
    "        \n",
    "        row_num += 1\n",
    "    \n",
    "    # Apply formatting to all cells\n",
    "    for row in ws.iter_rows(min_row=3, max_row=row_num-1):\n",
    "        for cell in row:\n",
    "            cell.border = Border(\n",
    "                left=Side(style='thin'),\n",
    "                right=Side(style='thin'),\n",
    "                top=Side(style='thin'),\n",
    "                bottom=Side(style='thin')\n",
    "            )\n",
    "            if isinstance(cell.value, float):\n",
    "                cell.number_format = '0.000'\n",
    "    \n",
    "    # Adjust column widths (skip merged cells)\n",
    "    column_widths = {}\n",
    "    for row in ws.iter_rows(min_row=3):  # Start from row 3 to skip merged cells\n",
    "        for cell in row:\n",
    "            if isinstance(cell, MergedCell):\n",
    "                continue\n",
    "            col = cell.column_letter\n",
    "            width = len(str(cell.value)) + 2\n",
    "            current_width = column_widths.get(col, 0)\n",
    "            column_widths[col] = max(current_width, width)\n",
    "    \n",
    "    # Apply the calculated widths\n",
    "    for col, width in column_widths.items():\n",
    "        ws.column_dimensions[col].width = width\n",
    "    \n",
    "    # Save Excel file\n",
    "    excel_filename = f'results_{data_name}_{recommender_name}.xlsx'\n",
    "    wb.save(excel_filename)\n",
    "    \n",
    "    # Return DataFrame for further analysis if needed\n",
    "    return df\n",
    "\n",
    "def print_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Print summary statistics for each method\n",
    "    \"\"\"\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    methods = df['Method'].unique()\n",
    "    metrics = ['DEL', 'INS', 'NDCG', 'POS_at_5', 'POS_at_10', 'POS_at_20']\n",
    "    \n",
    "    summary_rows = []\n",
    "    for method in methods:\n",
    "        method_data = df[df['Method'] == method]\n",
    "        row = {'Method': method}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            row[f'{metric}_Mean'] = method_data[metric].mean()\n",
    "            row[f'{metric}_Std'] = method_data[metric].std()\n",
    "        \n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(summary_df.round(3))\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "# Use the functions\n",
    "def generate_tables(results, data_name, recommender_name):\n",
    "    df = create_results_table(results, data_name, recommender_name)\n",
    "    summary_df = print_summary_statistics(df)\n",
    "    return df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11be6e8-ba23-4189-a90b-219e9f634b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ML1M dataset with MLP recommender\n",
      " ============ Start explaining ML1M MLP by jaccard ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1208 [00:00<?, ?it/s]/storage/mikhail/PI4Rec/code/recommenders_architecture.ipynb:42: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
      "  \"import logging\"\n",
      "100%|██████████| 1208/1208 [01:02<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_jaccard:  0.956402471365518\n",
      "users_INS_jaccard:  0.7284667452809629\n",
      "NDCG_jaccard:  0.9365028494228885\n",
      "POS_at_5_jaccard:  0.9839403973509933\n",
      "POS_at_10_jaccard:  0.9943708609271523\n",
      "POS_at_20_jaccard:  0.9973509933774835\n",
      "NEG_at_5_jaccard:  0.9996688741721853\n",
      "NEG_at_10_jaccard:  0.9996688741721853\n",
      "NEG_at_20_jaccard:  0.9998344370860927\n",
      " ============ Start explaining ML1M MLP by cosine ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [01:01<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_cosine:  0.9519159356341851\n",
      "users_INS_cosine:  0.7568457030327312\n",
      "NDCG_cosine:  0.8894647104850668\n",
      "POS_at_5_cosine:  0.9655629139072849\n",
      "POS_at_10_cosine:  0.9829470198675497\n",
      "POS_at_20_cosine:  0.9915562913907284\n",
      "NEG_at_5_cosine:  0.9996688741721853\n",
      "NEG_at_10_cosine:  1.0\n",
      "NEG_at_20_cosine:  1.0\n",
      " ============ Start explaining ML1M MLP by lime ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [01:01<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_lime:  0.9469477439311561\n",
      "users_INS_lime:  0.7595790430261994\n",
      "NDCG_lime:  0.8849987563263897\n",
      "POS_at_5_lime:  0.9675496688741722\n",
      "POS_at_10_lime:  0.9832781456953642\n",
      "POS_at_20_lime:  0.9917218543046358\n",
      "NEG_at_5_lime:  0.9991721854304636\n",
      "NEG_at_10_lime:  1.0\n",
      "NEG_at_20_lime:  1.0\n",
      " ============ Start explaining ML1M MLP by lxr ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [01:17<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_lxr:  0.9434245973874795\n",
      "users_INS_lxr:  0.784139019255804\n",
      "NDCG_lxr:  0.8742300574982305\n",
      "POS_at_5_lxr:  0.9561258278145696\n",
      "POS_at_10_lxr:  0.9763245033112583\n",
      "POS_at_20_lxr:  0.9864238410596026\n",
      "NEG_at_5_lxr:  0.9990066225165563\n",
      "NEG_at_10_lxr:  0.9998344370860927\n",
      "NEG_at_20_lxr:  1.0\n",
      " ============ Start explaining ML1M MLP by accent ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [01:18<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_accent:  0.9502151558533409\n",
      "users_INS_accent:  0.7583083641203429\n",
      "NDCG_accent:  0.932857665158068\n",
      "POS_at_5_accent:  0.9831125827814569\n",
      "POS_at_10_accent:  0.991225165562914\n",
      "POS_at_20_accent:  0.9942052980132451\n",
      "NEG_at_5_accent:  0.984933774834437\n",
      "NEG_at_10_accent:  0.9948675496688741\n",
      "NEG_at_20_accent:  0.9975165562913907\n",
      " ============ Start explaining ML1M MLP by shap ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [01:04<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_DEL_shap:  0.9709385250873912\n",
      "users_INS_shap:  0.6460185138348318\n",
      "NDCG_shap:  0.9594799328472047\n",
      "POS_at_5_shap:  0.997682119205298\n",
      "POS_at_10_shap:  0.9996688741721853\n",
      "POS_at_20_shap:  0.9998344370860927\n",
      "NEG_at_5_shap:  0.9963576158940397\n",
      "NEG_at_10_shap:  0.9980132450331125\n",
      "NEG_at_20_shap:  0.9986754966887418\n",
      "Saved plot: AUC_DEL-PatK_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_INS-PatK_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NDCG-P_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat5_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat10_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat20_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat5_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat10_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat20_ML1M_MLPd.pdf\n",
      "\n",
      "Processing complete. Results and visualizations have been saved to plots_discrete directory.\n"
     ]
    }
   ],
   "source": [
    "data_names = [\"ML1M\"]#, ,\"Yahoo\",\"Pinterest\"\n",
    "recommender_names = [ \"MLP\"]#\"MLP\"\"VAE\", \"NCF\"\n",
    "\n",
    "# Create a mapping between explainer names and actual explainer functions\n",
    "explainer_mapping = {\n",
    "   # 'pop': find_pop_mask,\n",
    "    'jaccard': find_jaccard_mask,\n",
    "    'cosine': find_cosine_mask,\n",
    "    'lime': find_lime_mask,\n",
    "    'lire': find_lire_mask,\n",
    "    'lxr': find_lxr_mask,\n",
    "    'accent': find_accent_mask,\n",
    "    'shap': find_shapley_mask\n",
    "}\n",
    "\n",
    "# Initialize storage for all results\n",
    "# Initialize storage for all results\n",
    "all_results = {}\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = Path('plots_discrete')\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for data_name in data_names:\n",
    "    DP_DIR = Path(\"processed_data\", data_name)\n",
    "    files_path = Path(\"/storage/mikhail/PI4Rec\", DP_DIR)\n",
    "\n",
    "    # Load dataset-specific parameters and data\n",
    "    num_users = num_users_dict[data_name] \n",
    "    num_items = num_items_dict[data_name] \n",
    "    num_features = num_items_dict[data_name]\n",
    "        \n",
    "    with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "        pop_dict = pickle.load(f)\n",
    "    pop_array = np.zeros(len(pop_dict))\n",
    "    for key, value in pop_dict.items():\n",
    "        pop_array[key] = value\n",
    "\n",
    "    # Load data files\n",
    "    train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "    test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "    static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "    \n",
    "    train_array = train_data.to_numpy()\n",
    "    test_array = test_data.to_numpy()\n",
    "    items_array = np.eye(num_items)\n",
    "    all_items_tensor = torch.Tensor(items_array).to(device)\n",
    "    test_array = static_test_data.iloc[:,:-2].to_numpy()\n",
    "\n",
    "    for recommender_name in recommender_names:\n",
    "        print(f\"\\nProcessing {data_name} dataset with {recommender_name} recommender\")\n",
    "        \n",
    "        # Set up recommender-specific parameters\n",
    "        output_type = output_type_dict[recommender_name]\n",
    "        hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "        recommender_path = recommender_path_dict[(data_name,recommender_name)]\n",
    "\n",
    "        kw_dict = {\n",
    "            'device': device,\n",
    "            'num_items': num_items,\n",
    "            'demographic': False,\n",
    "            'num_features': num_features,\n",
    "            'pop_array': pop_array,\n",
    "            'all_items_tensor': all_items_tensor,\n",
    "            'static_test_data': static_test_data,\n",
    "            'items_array': items_array,\n",
    "            'output_type': output_type,\n",
    "            'recommender_name': recommender_name\n",
    "        }\n",
    "\n",
    "        recommender = load_recommender()\n",
    "\n",
    "        # Process each explanation method\n",
    "        results = {}\n",
    "        for expl_name in ['jaccard', 'cosine', 'lime', 'lxr', 'accent', 'shap']:\n",
    "            try:\n",
    "                results[expl_name] = eval_one_expl_type(expl_name)\n",
    "                # Take only first 5 values from each metric array\n",
    "                results[expl_name] = {\n",
    "                    metric: values[:5] if len(values) >= 5 else values \n",
    "                    for metric, values in results[expl_name].items()\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {expl_name} for {data_name} {recommender_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Store results in the overall dictionary\n",
    "        all_results[(data_name, recommender_name)] = results\n",
    "        \n",
    "        # Generate and save plots\n",
    "        plot_all_metrics(results, data_name, recommender_name)\n",
    "\n",
    "print(\"\\nProcessing complete. Results and visualizations have been saved to plots_discrete directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe4f9310-964a-4ce5-b4ae-046279e30d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: AUC_DEL-PatK_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_INS-PatK_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NDCG-P_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat5_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat10_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_POS-Pat20_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat5_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat10_ML1M_MLPd.pdf\n",
      "Saved plot: AUC_NEG-Pat20_ML1M_MLPd.pdf\n"
     ]
    }
   ],
   "source": [
    "plot_all_metrics(results, data_name, recommender_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c0b4f-e22a-4544-b38a-69b5639d4ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "219789d9-5fa4-4fc4-b4c7-2999c99616e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Step</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>DEL</th>\n",
       "      <th>INS</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>POS_at_5</th>\n",
       "      <th>POS_at_10</th>\n",
       "      <th>POS_at_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JACCARD</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.970001</td>\n",
       "      <td>0.666879</td>\n",
       "      <td>0.973947</td>\n",
       "      <td>0.998344</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JACCARD</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.963509</td>\n",
       "      <td>0.703173</td>\n",
       "      <td>0.954638</td>\n",
       "      <td>0.995033</td>\n",
       "      <td>0.997517</td>\n",
       "      <td>0.998344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JACCARD</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.956697</td>\n",
       "      <td>0.733510</td>\n",
       "      <td>0.937319</td>\n",
       "      <td>0.989238</td>\n",
       "      <td>0.995033</td>\n",
       "      <td>0.997517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JACCARD</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.949667</td>\n",
       "      <td>0.758765</td>\n",
       "      <td>0.916401</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.992550</td>\n",
       "      <td>0.995861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JACCARD</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.942137</td>\n",
       "      <td>0.780008</td>\n",
       "      <td>0.900209</td>\n",
       "      <td>0.963576</td>\n",
       "      <td>0.987583</td>\n",
       "      <td>0.995861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>COSINE</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.968171</td>\n",
       "      <td>0.685167</td>\n",
       "      <td>0.948745</td>\n",
       "      <td>0.997517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COSINE</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.960639</td>\n",
       "      <td>0.730850</td>\n",
       "      <td>0.914627</td>\n",
       "      <td>0.986755</td>\n",
       "      <td>0.995861</td>\n",
       "      <td>0.998344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COSINE</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.952149</td>\n",
       "      <td>0.765268</td>\n",
       "      <td>0.882621</td>\n",
       "      <td>0.964404</td>\n",
       "      <td>0.987583</td>\n",
       "      <td>0.993377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COSINE</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.943702</td>\n",
       "      <td>0.791354</td>\n",
       "      <td>0.860932</td>\n",
       "      <td>0.944536</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.987583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COSINE</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.934919</td>\n",
       "      <td>0.811589</td>\n",
       "      <td>0.840398</td>\n",
       "      <td>0.934603</td>\n",
       "      <td>0.957781</td>\n",
       "      <td>0.978477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LIME</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.966912</td>\n",
       "      <td>0.683917</td>\n",
       "      <td>0.953021</td>\n",
       "      <td>0.998344</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LIME</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.957443</td>\n",
       "      <td>0.731011</td>\n",
       "      <td>0.915986</td>\n",
       "      <td>0.986755</td>\n",
       "      <td>0.994205</td>\n",
       "      <td>0.996689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LIME</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.947113</td>\n",
       "      <td>0.767866</td>\n",
       "      <td>0.883792</td>\n",
       "      <td>0.964404</td>\n",
       "      <td>0.987583</td>\n",
       "      <td>0.993377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LIME</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.936840</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>0.850133</td>\n",
       "      <td>0.947848</td>\n",
       "      <td>0.975993</td>\n",
       "      <td>0.987583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LIME</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.926430</td>\n",
       "      <td>0.818770</td>\n",
       "      <td>0.822061</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.959437</td>\n",
       "      <td>0.981788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LXR</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.967427</td>\n",
       "      <td>0.688064</td>\n",
       "      <td>0.956157</td>\n",
       "      <td>0.997517</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LXR</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.957084</td>\n",
       "      <td>0.748438</td>\n",
       "      <td>0.915628</td>\n",
       "      <td>0.989238</td>\n",
       "      <td>0.995861</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LXR</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.944787</td>\n",
       "      <td>0.794631</td>\n",
       "      <td>0.869593</td>\n",
       "      <td>0.957781</td>\n",
       "      <td>0.981788</td>\n",
       "      <td>0.992550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LXR</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.931200</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.832211</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.958609</td>\n",
       "      <td>0.979305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LXR</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.916624</td>\n",
       "      <td>0.858760</td>\n",
       "      <td>0.797562</td>\n",
       "      <td>0.902318</td>\n",
       "      <td>0.946192</td>\n",
       "      <td>0.961921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ACCENT</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.968516</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.974987</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ACCENT</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.959993</td>\n",
       "      <td>0.731201</td>\n",
       "      <td>0.953234</td>\n",
       "      <td>0.994205</td>\n",
       "      <td>0.997517</td>\n",
       "      <td>0.998344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ACCENT</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.950827</td>\n",
       "      <td>0.766603</td>\n",
       "      <td>0.932915</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>0.992550</td>\n",
       "      <td>0.995033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ACCENT</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.940978</td>\n",
       "      <td>0.794108</td>\n",
       "      <td>0.910137</td>\n",
       "      <td>0.976821</td>\n",
       "      <td>0.986755</td>\n",
       "      <td>0.990894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ACCENT</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.930762</td>\n",
       "      <td>0.816129</td>\n",
       "      <td>0.893015</td>\n",
       "      <td>0.961093</td>\n",
       "      <td>0.980132</td>\n",
       "      <td>0.987583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SHAP</td>\n",
       "      <td>1</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.974701</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.978312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SHAP</td>\n",
       "      <td>2</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.973309</td>\n",
       "      <td>0.634808</td>\n",
       "      <td>0.969874</td>\n",
       "      <td>0.999172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SHAP</td>\n",
       "      <td>3</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.971281</td>\n",
       "      <td>0.646004</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.996689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SHAP</td>\n",
       "      <td>4</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.969286</td>\n",
       "      <td>0.656120</td>\n",
       "      <td>0.952581</td>\n",
       "      <td>0.995861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SHAP</td>\n",
       "      <td>5</td>\n",
       "      <td>ML1M</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.966116</td>\n",
       "      <td>0.667866</td>\n",
       "      <td>0.936368</td>\n",
       "      <td>0.996689</td>\n",
       "      <td>0.998344</td>\n",
       "      <td>0.999172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Step Dataset Recommender       DEL       INS      NDCG  POS_at_5  \\\n",
       "0   JACCARD     1    ML1M         MLP  0.970001  0.666879  0.973947  0.998344   \n",
       "1   JACCARD     2    ML1M         MLP  0.963509  0.703173  0.954638  0.995033   \n",
       "2   JACCARD     3    ML1M         MLP  0.956697  0.733510  0.937319  0.989238   \n",
       "3   JACCARD     4    ML1M         MLP  0.949667  0.758765  0.916401  0.973510   \n",
       "4   JACCARD     5    ML1M         MLP  0.942137  0.780008  0.900209  0.963576   \n",
       "5    COSINE     1    ML1M         MLP  0.968171  0.685167  0.948745  0.997517   \n",
       "6    COSINE     2    ML1M         MLP  0.960639  0.730850  0.914627  0.986755   \n",
       "7    COSINE     3    ML1M         MLP  0.952149  0.765268  0.882621  0.964404   \n",
       "8    COSINE     4    ML1M         MLP  0.943702  0.791354  0.860932  0.944536   \n",
       "9    COSINE     5    ML1M         MLP  0.934919  0.811589  0.840398  0.934603   \n",
       "10     LIME     1    ML1M         MLP  0.966912  0.683917  0.953021  0.998344   \n",
       "11     LIME     2    ML1M         MLP  0.957443  0.731011  0.915986  0.986755   \n",
       "12     LIME     3    ML1M         MLP  0.947113  0.767866  0.883792  0.964404   \n",
       "13     LIME     4    ML1M         MLP  0.936840  0.796330  0.850133  0.947848   \n",
       "14     LIME     5    ML1M         MLP  0.926430  0.818770  0.822061  0.940397   \n",
       "15      LXR     1    ML1M         MLP  0.967427  0.688064  0.956157  0.997517   \n",
       "16      LXR     2    ML1M         MLP  0.957084  0.748438  0.915628  0.989238   \n",
       "17      LXR     3    ML1M         MLP  0.944787  0.794631  0.869593  0.957781   \n",
       "18      LXR     4    ML1M         MLP  0.931200  0.830801  0.832211  0.933775   \n",
       "19      LXR     5    ML1M         MLP  0.916624  0.858760  0.797562  0.902318   \n",
       "20   ACCENT     1    ML1M         MLP  0.968516  0.683500  0.974987  0.999172   \n",
       "21   ACCENT     2    ML1M         MLP  0.959993  0.731201  0.953234  0.994205   \n",
       "22   ACCENT     3    ML1M         MLP  0.950827  0.766603  0.932915  0.984272   \n",
       "23   ACCENT     4    ML1M         MLP  0.940978  0.794108  0.910137  0.976821   \n",
       "24   ACCENT     5    ML1M         MLP  0.930762  0.816129  0.893015  0.961093   \n",
       "25     SHAP     1    ML1M         MLP  0.974701  0.625295  0.978312  1.000000   \n",
       "26     SHAP     2    ML1M         MLP  0.973309  0.634808  0.969874  0.999172   \n",
       "27     SHAP     3    ML1M         MLP  0.971281  0.646004  0.960264  0.996689   \n",
       "28     SHAP     4    ML1M         MLP  0.969286  0.656120  0.952581  0.995861   \n",
       "29     SHAP     5    ML1M         MLP  0.966116  0.667866  0.936368  0.996689   \n",
       "\n",
       "    POS_at_10  POS_at_20  \n",
       "0    0.999172   0.999172  \n",
       "1    0.997517   0.998344  \n",
       "2    0.995033   0.997517  \n",
       "3    0.992550   0.995861  \n",
       "4    0.987583   0.995861  \n",
       "5    1.000000   1.000000  \n",
       "6    0.995861   0.998344  \n",
       "7    0.987583   0.993377  \n",
       "8    0.973510   0.987583  \n",
       "9    0.957781   0.978477  \n",
       "10   0.999172   0.999172  \n",
       "11   0.994205   0.996689  \n",
       "12   0.987583   0.993377  \n",
       "13   0.975993   0.987583  \n",
       "14   0.959437   0.981788  \n",
       "15   0.999172   0.999172  \n",
       "16   0.995861   0.999172  \n",
       "17   0.981788   0.992550  \n",
       "18   0.958609   0.979305  \n",
       "19   0.946192   0.961921  \n",
       "20   0.999172   0.999172  \n",
       "21   0.997517   0.998344  \n",
       "22   0.992550   0.995033  \n",
       "23   0.986755   0.990894  \n",
       "24   0.980132   0.987583  \n",
       "25   1.000000   1.000000  \n",
       "26   1.000000   1.000000  \n",
       "27   1.000000   1.000000  \n",
       "28   1.000000   1.000000  \n",
       "29   0.998344   0.999172  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_results_table(results, data_name, recommender_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1976c-7bbf-48d0-8545-322c1d85d599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401d52d-d3d1-49fe-85b7-5eea74e2a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
