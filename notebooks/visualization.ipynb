{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for loading the results\n",
    "# In a real scenario, you would load the results from a file\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics(results, data_name, recommender_name):\n",
    "    # Mapping between metric keys and their display names (title, y-label, indicator)\n",
    "    metrics_mapping = {\n",
    "        'DEL':      ('AUC DEL-P@K', 'DEL@Kₑ', 'Lower is better'),\n",
    "        'INS':      ('AUC INS-P@K', 'INS@Kₑ', 'Higher is better'),\n",
    "        'NDCG':     ('AUC NDCG-P',  'CNDCG@Kₑ', 'Lower is better'),\n",
    "        'POS_at_5': ('AUC POS-P@5', 'POS@20Kₑ', 'Lower is better'),\n",
    "        'POS_at_10':('AUC POS-P@10','POS@20Kₑ', 'Lower is better'),\n",
    "        'POS_at_20':('AUC POS-P@20','POS@20Kₑ', 'Lower is better'),\n",
    "        'NEG_at_5': ('AUC NEG-P@5', 'NEG@20Kₑ', 'Higher is better'),\n",
    "        'NEG_at_10':('AUC NEG-P@10','NEG@20Kₑ', 'Higher is better'),\n",
    "        'NEG_at_20':('AUC NEG-P@20','NEG@20Kₑ', 'Higher is better')\n",
    "    }\n",
    "    \n",
    "    # Style lists\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'x']\n",
    "    linestyles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 2))]\n",
    "    \n",
    "    # Create plots directory\n",
    "    os.makedirs('results/plots', exist_ok=True)\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric, (title_name, y_label, indicator) in metrics_mapping.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for i, (baseline, baseline_metrics) in enumerate(results.items()):\n",
    "            if metric not in baseline_metrics:\n",
    "                print(f\"Warning: {metric} not found in {baseline} metrics\")\n",
    "                continue\n",
    "            \n",
    "            values = baseline_metrics[metric][:5]  # Take only first 5 values\n",
    "            x = range(1, len(values) + 1)  # Numbers of masked items (1, 2, 3, 4, 5)\n",
    "            \n",
    "            plt.plot(\n",
    "                x, values,\n",
    "                label=baseline.upper(),\n",
    "                color=colors[i % len(colors)],\n",
    "                linestyle=linestyles[i % len(linestyles)],\n",
    "                marker=markers[i % len(markers)],\n",
    "                markersize=8,\n",
    "                linewidth=2,\n",
    "                markevery=1  # Markers on each value\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(\"Number of Masked Items\", fontsize=30)\n",
    "        plt.ylabel(y_label, fontsize=30)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5)\n",
    "        plt.xticks(range(1, 6), fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.legend(\n",
    "            loc='best', \n",
    "            fontsize=20,\n",
    "            frameon=True,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        safe_display_name = title_name.replace(\" \", \"_\").replace(\"@\", \"at\")\n",
    "        plt.savefig(f'results/plots/{safe_display_name}_{data_name}_{recommender_name}d.pdf',\n",
    "                    format='pdf', bbox_inches='tight')\n",
    "        print(f\"Saved plot: results/plots/{safe_display_name}_{data_name}_{recommender_name}d.pdf\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_visualizations(all_results, save_dir='results/plots'): \n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations comparing methods across datasets and recommenders\n",
    "    \n",
    "    Parameters:\n",
    "    all_results: dict\n",
    "        Format: {(dataset_name, recommender_name): results_dict}\n",
    "        where results_dict contains metrics for each explanation method\n",
    "    save_dir: str\n",
    "        Directory to save the visualization files\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for (dataset, recommender), results in all_results.items():\n",
    "        for method, metrics in results.items():\n",
    "            for metric_name, values in metrics.items():\n",
    "                if isinstance(values, np.ndarray):\n",
    "                    for step, value in enumerate(values, 1):\n",
    "                        plot_data.append({\n",
    "                            'Dataset': dataset,\n",
    "                            'Recommender': recommender,\n",
    "                            'Method': method.upper(),\n",
    "                            'Metric': metric_name,\n",
    "                            'Step': step,\n",
    "                            'Value': value\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # 1. Heatmap of method performance across datasets and recommenders\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics_to_plot = ['DEL', 'INS', 'NDCG']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(1, 3, idx+1)\n",
    "        pivot_data = df[df['Metric'] == metric].groupby(\n",
    "            ['Dataset', 'Recommender', 'Method'])['Value'].mean().unstack()\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "        plt.title(f'{metric} Performance Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/performance_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Method stability analysis (standard deviation across steps)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    stability_data = df.groupby(['Method', 'Metric'])['Value'].std().unstack()\n",
    "    stability_data.plot(kind='bar', width=0.8)\n",
    "    plt.title('Method Stability Analysis (Standard Deviation Across Steps)')\n",
    "    plt.xlabel('Explanation Method')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/method_stability.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Radar chart for method comparison\n",
    "    def create_radar_chart(data, methods, metrics):\n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        for method in methods:\n",
    "            values = [data[(data['Method'] == method) & \n",
    "                          (data['Metric'] == metric)]['Value'].mean() \n",
    "                     for metric in metrics]\n",
    "            values += values[:1]\n",
    "            angles_plot = np.concatenate([angles, [angles[0]]])\n",
    "            ax.plot(angles_plot, values, 'o-', label=method)\n",
    "            ax.fill(angles_plot, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    metrics_for_radar = ['DEL', 'INS', 'NDCG', 'POS_at_5', 'POS_at_10']\n",
    "    for dataset in df['Dataset'].unique():\n",
    "        for recommender in df['Recommender'].unique():\n",
    "            data_subset = df[(df['Dataset'] == dataset) & \n",
    "                           (df['Recommender'] == recommender)]\n",
    "            fig = create_radar_chart(data_subset, \n",
    "                                   df['Method'].unique(), \n",
    "                                   metrics_for_radar)\n",
    "            plt.title(f'{dataset} - {recommender}\\nMethod Comparison')\n",
    "            plt.savefig(f'{save_dir}/radar_{dataset}_{recommender}.png')\n",
    "            plt.close()\n",
    "\n",
    "    # 4. Box plots showing distribution of metrics across steps\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for idx, metric in enumerate(['DEL', 'INS', 'NDCG'], 1):\n",
    "        plt.subplot(1, 3, idx)\n",
    "        sns.boxplot(data=df[df['Metric'] == metric], \n",
    "                   x='Method', y='Value', \n",
    "                   hue='Dataset')\n",
    "        plt.title(f'{metric} Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/metric_distributions.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Performance improvement over steps\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for metric in ['DEL', 'INS', 'NDCG']:\n",
    "        plt.subplot(1, 3, metrics_to_plot.index(metric) + 1)\n",
    "        for method in df['Method'].unique():\n",
    "            data = df[(df['Metric'] == metric) & (df['Method'] == method)]\n",
    "            plt.plot(data.groupby('Step')['Value'].mean(), \n",
    "                    marker='o', \n",
    "                    label=method)\n",
    "        plt.title(f'{metric} Progress Over Steps')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Value')\n",
    "        if metric == 'NDCG':\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/progress_over_steps.png')\n",
    "    plt.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(df, save_dir='results/tables'):\n",
    "    \"\"\"\n",
    "    Generates a statistical summary report of the results\n",
    "    \"\"\"\n",
    "    # Calculate aggregate statistics\n",
    "    summary = df.groupby(['Dataset', 'Recommender', 'Method', 'Metric'])['Value'].agg([\n",
    "        'mean', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary.to_csv(f'{save_dir}/summary_statistics.csv')\n",
    "    \n",
    "    # Calculate method rankings\n",
    "    rankings = df.groupby(['Dataset', 'Recommender', 'Method', 'Metric'])['Value'].mean().unstack()\n",
    "    method_ranks = rankings.rank(ascending=False, method='min')\n",
    "    method_ranks.to_csv(f'{save_dir}/method_rankings.csv')\n",
    "    \n",
    "    return summary, method_ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}