{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b69078",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160299a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ipynb\n",
    "import wandb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60d9f2b-37ce-4c7c-9d2e-8e0a7d8d1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "recommender_name = \"NCF\" ### Can be MLP, VAE, NCF\n",
    "\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(\"/storage/mikhail/PI4Rec\", DP_DIR)\n",
    "checkpoints_path = Path(export_dir.parent, \"checkpoints\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56467a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\"\n",
    "}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155\n",
    "}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP_ML1M_0.002_1024_19_8.pt\"),\n",
    "    (\"ML1M\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_5e-05_64_16.pt\"),\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    (\"Yahoo\",\"NCF\"):Path(checkpoints_path, \"NCF_Yahoo_0.001_64_21_0.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_12_18_0.0001_256.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    (\"Pinterest\",\"NCF\"):Path(checkpoints_path, \"NCF2_Pinterest_9e-05_32_9_10.pt\"),}\n",
    "\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 512,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"NCF\"): 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a2c01f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NCF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_type \u001b[38;5;241m=\u001b[39m \u001b[43moutput_type_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecommender_name\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m### Can be single, multiple\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_users \u001b[38;5;241m=\u001b[39m num_users_dict[data_name] \n\u001b[1;32m      3\u001b[0m num_items \u001b[38;5;241m=\u001b[39m num_items_dict[data_name] \n",
      "\u001b[0;31mKeyError\u001b[0m: 'NCF'"
     ]
    }
   ],
   "source": [
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] \n",
    "\n",
    "hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "recommender_path = recommender_path_dict[(data_name,recommender_name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6b87",
   "metadata": {},
   "source": [
    "## Data imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "train_data['user_id'] = train_data.index\n",
    "test_data['user_id'] = test_data.index\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdd7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db85e",
   "metadata": {},
   "source": [
    "# Recommenders Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd36660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_config= {\n",
    "\"enc_dims\": [512,128],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name=='MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='VAE':\n",
    "        recommender = VAE(VAE_config, **kw_dict)\n",
    "\n",
    "    recommender_checkpoint = torch.load(Path(checkpoints_path, recommender_path))\n",
    "    recommender.load_state_dict(recommender_checkpoint)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad= False\n",
    "    return recommender\n",
    "    \n",
    "recommender = load_recommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc79dc",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba496f",
   "metadata": {},
   "source": [
    "## Load / create top recommended items dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dicts = False\n",
    "if create_dicts:\n",
    "    top1_train = {}\n",
    "    top1_test = {}\n",
    "    for i in range(train_array.shape[0]):\n",
    "        user_index = train_array[i][-1]\n",
    "        user_tensor = torch.Tensor(train_array[i][:-1]).to(device)\n",
    "        top1_train[user_index] = int(get_user_recommended_item(user_tensor, recommender, **kw_dict))\n",
    "    for i in range(test_array.shape[0]):\n",
    "        user_index = test_array[i][-1]\n",
    "        user_tensor = torch.Tensor(test_array[i][:-1]).to(device)\n",
    "        top1_test[user_index] = int(get_user_recommended_item(user_tensor, recommender, **kw_dict))\n",
    "        \n",
    "    with open(Path(files_path,f'top1_train_{data_name}_{recommender_name}.pkl'), 'wb') as f:\n",
    "        pickle.dump(top1_train, f)\n",
    "    with open(Path(files_path,f'top1_test_{data_name}_{recommender_name}.pkl'), 'wb') as f:\n",
    "        pickle.dump(top1_test, f)\n",
    "else:\n",
    "    with open(Path(files_path,f'top1_train_{data_name}_{recommender_name}.pkl'), 'rb') as f:\n",
    "        top1_train = pickle.load(f)\n",
    "    with open(Path(files_path,f'top1_test_{data_name}_{recommender_name}.pkl'), 'rb') as f:\n",
    "        top1_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f47d6",
   "metadata": {},
   "source": [
    "# LXR Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c44742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(nn.Module):\n",
    "    def __init__(self, user_size, item_size, hidden_size):\n",
    "        super(Explainer, self).__init__()\n",
    "        \n",
    "        self.users_fc = nn.Linear(in_features = user_size, out_features=hidden_size).to(device)\n",
    "        self.items_fc = nn.Linear(in_features = item_size, out_features=hidden_size).to(device)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size).to(device),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size, out_features=user_size).to(device),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_output = self.users_fc(user_tensor.float())\n",
    "        item_output = self.items_fc(item_tensor.float())\n",
    "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
    "        expl_scores = self.bottleneck(combined_output).to(device)\n",
    "        return expl_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3557f3",
   "metadata": {},
   "source": [
    "# Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LXR_loss(nn.Module):\n",
    "    def __init__(self, lambda_pos, lambda_neg, alpha):\n",
    "        super(LXR_loss, self).__init__()\n",
    "        \n",
    "        self.lambda_pos = lambda_pos\n",
    "        self.lambda_neg = lambda_neg\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "    def forward(self, user_tensors, items_tensors, items_ids, pos_masks):\n",
    "        neg_masks = torch.sub(torch.ones_like(pos_masks), pos_masks)\n",
    "        x_masked_pos = user_tensors * pos_masks\n",
    "        x_masked_neg = user_tensors * neg_masks\n",
    "        if output_type=='single':\n",
    "            x_masked_res_pos = torch.diag(recommender_run(x_masked_pos, recommender, items_tensors, item_id=items_ids, wanted_output = 'single', **kw_dict))\n",
    "            x_masked_res_neg = torch.diag(recommender_run(x_masked_neg, recommender, items_tensors, item_id=items_ids, wanted_output = 'single', **kw_dict))\n",
    "        else:\n",
    "            x_masked_res_pos_before = recommender_run(x_masked_pos, recommender, items_tensors, item_id=items_ids, wanted_output = 'vector', **kw_dict)\n",
    "            x_masked_res_neg_before = recommender_run(x_masked_neg, recommender, items_tensors, item_id=items_ids, wanted_output = 'vector', **kw_dict)\n",
    "            rows=torch.arange(len(items_ids))\n",
    "            x_masked_res_pos = x_masked_res_pos_before[rows, items_ids] \n",
    "            x_masked_res_neg = x_masked_res_neg_before[rows, items_ids] \n",
    "            \n",
    "            \n",
    "        pos_loss = -torch.mean(torch.log(x_masked_res_pos))\n",
    "        neg_loss = torch.mean(torch.log(x_masked_res_neg))\n",
    "        l1 = x_masked_pos[user_tensors>0].mean()\n",
    "        combined_loss = self.lambda_pos*pos_loss + self.lambda_neg*neg_loss + self.alpha*l1\n",
    "        \n",
    "        return combined_loss, pos_loss, neg_loss, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16aeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LXR based similarity\n",
    "def find_LXR_mask(user_tensor, item_id, item_tensor, explainer):\n",
    "    expl_scores = explainer(user_tensor, item_tensor)\n",
    "    x_masked = user_tensor*expl_scores\n",
    "    item_sim_dict = {i: x_masked[i].item() for i in range(len(x_masked))}    \n",
    "\n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4835e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate LXR pos@20 and neg@20 scores on test set\n",
    "def calculate_pos_neg_k(user_tensor, item_id, items_tensor, num_of_bins, explainer, k=20):\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    user_hist_size = int(torch.sum(user_tensor))\n",
    "\n",
    "    bins = [0] + [len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "\n",
    "    POS_at_20 = [0] * (num_of_bins+1)\n",
    "    NEG_at_20 = [0] * (num_of_bins+1)\n",
    "    total_items = 0\n",
    "    \n",
    "    #returns original tensor\n",
    "    sim_items = find_LXR_mask(user_tensor, item_id, items_tensor, explainer)\n",
    "    POS_sim_items=list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "    NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked # remove the masked items from the user history \n",
    "        \n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "        \n",
    "        POS_index = get_index_in_the_list(POS_masked, user_tensor, item_id, recommender, **kw_dict)+1\n",
    "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, recommender, **kw_dict)+1        \n",
    "        \n",
    "        POS_at_20[i] = 1 if POS_index <= 20 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "\n",
    "    res = [np.array(POS_at_20), np.array(NEG_at_20)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befb216",
   "metadata": {},
   "source": [
    "# LXR training\n",
    "### Utilizing Optuna for hyperparameter optimization and WandB (Weights and Biases) for experiment tracking and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2db824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "num_of_rand_users = 200 # number of users for evaluations\n",
    "random_rows = np.random.choice(test_array.shape[0], num_of_rand_users, replace=False)\n",
    "random_sampled_array = test_array[random_rows]\n",
    "\n",
    "def lxr_training(trial):\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
    "    alpha = trial.suggest_categorical('alpha', [1]) # set alpha to be 1, change other hyperparameters\n",
    "    lambda_neg = trial.suggest_float('lambda_neg', 0,50)\n",
    "    lambda_pos = trial.suggest_float('lambda_pos', 0,50)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32,64,128,256])\n",
    "    explainer_hidden_size = trial.suggest_categorical('explainer_hidden_size', [32,64,128])\n",
    "    epochs = 40\n",
    "    num_features = num_items\n",
    "    wandb.init(\n",
    "        project=f\"{data_name}_{recommender_name}_LXR_training\",\n",
    "        name=f\"trial_{trial.number}\",\n",
    "        config={\n",
    "        'learning_rate' : learning_rate,\n",
    "        'alpha' : alpha,\n",
    "        'lambda_neg' : lambda_neg,\n",
    "        'lambda_pos' : lambda_pos,\n",
    "        'batch_size' : batch_size,\n",
    "        'explainer_hidden_size' : explainer_hidden_size,\n",
    "        'architecture' : 'LXR_combined',\n",
    "        'activation_function' : 'Tanh',\n",
    "        'loss_type' : 'logloss',\n",
    "        'optimize_for' : 'pos_at_20',\n",
    "        'epochs':epochs\n",
    "        })\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(train_array, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = int(np.ceil(train_array.shape[0] / batch_size))\n",
    "\n",
    "\n",
    "    num_of_bins = 10\n",
    "    run_pos_at_20 = []\n",
    "    run_neg_at_20 = []\n",
    "    metric_for_monitoring = []\n",
    "    train_losses = []\n",
    "\n",
    "    recommender.eval()\n",
    "    explainer = Explainer(num_features, num_items, explainer_hidden_size).to(device) \n",
    "    optimizer_comb = torch.optim.Adam(explainer.parameters(), learning_rate)\n",
    "    loss_func = LXR_loss(lambda_pos, lambda_neg, alpha)\n",
    "\n",
    "    print('======================== new run ========================')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%15 == 0 and epoch>0: # decrease learning rate every 15 epochs\n",
    "            learning_rate*= 0.1\n",
    "            optimizer_comb.lr = learning_rate\n",
    "\n",
    "        POS_at_20_lxr = np.zeros(num_of_bins+1)\n",
    "        NEG_at_20_lxr = np.zeros(num_of_bins+1)\n",
    "        train_loss = 0\n",
    "        total_pos_loss=0\n",
    "        total_neg_loss=0\n",
    "        total_l1_loss=0\n",
    "\n",
    "        explainer.train()\n",
    "        for batch_index, samples in enumerate(loader):\n",
    "            # prepare data for explainer:\n",
    "            user_tensors = torch.Tensor(samples[:,:-1]).to(device)\n",
    "            user_ids = samples[:,-1]\n",
    "            top1_item = np.array([top1_train[int(x)] for x in user_ids])\n",
    "            items_vectors = items_array[top1_item]\n",
    "            items_tensors = torch.Tensor(items_vectors).to(device)\n",
    "            n = user_tensors.shape[0]\n",
    "\n",
    "            # zero grad:\n",
    "            optimizer_comb.zero_grad()\n",
    "            # forward:\n",
    "            expl_scores = explainer(user_tensors, items_tensors)\n",
    "\n",
    "            # caclulate loss\n",
    "            comb_loss, pos_loss, neg_loss, l1 = loss_func(user_tensors, items_tensors, top1_item, expl_scores)\n",
    "            train_loss += comb_loss*n\n",
    "            total_pos_loss += pos_loss*n\n",
    "            total_neg_loss += neg_loss*n\n",
    "            total_l1_loss += l1*n\n",
    "\n",
    "            # back propagation\n",
    "            comb_loss.backward()\n",
    "            optimizer_comb.step()\n",
    "\n",
    "        train_metrics = {\"train/train_loss\": train_loss,\n",
    "                         \"train/pos_loss\": total_pos_loss,\n",
    "                         \"train/neg_loss\": total_neg_loss,\n",
    "                         \"train/l1_loss\": total_l1_loss,\n",
    "                         \"train/epoch\": epoch}\n",
    "\n",
    "        torch.save(explainer.state_dict(), Path(checkpoints_path, f'LXR_{data_name}_{recommender_name}_{trial.number}_{epoch}_{explainer_hidden_size}_{lambda_pos}_{lambda_neg}.pt'))\n",
    "\n",
    "        #Monitoring on POS metric after each epoch\n",
    "        explainer.eval()\n",
    "        for j in range(random_sampled_array.shape[0]):\n",
    "            user_id = random_sampled_array[j][-1]\n",
    "            user_tensor = torch.Tensor(random_sampled_array[j][:-1]).to(device)\n",
    "            top1_item_test = top1_test[user_id]\n",
    "            item_vector = items_array[top1_item_test]\n",
    "            items_tensor = torch.Tensor(item_vector).to(device)\n",
    "\n",
    "            res = calculate_pos_neg_k(user_tensor, top1_item_test, items_tensor, num_of_bins, explainer, k=20)\n",
    "            POS_at_20_lxr += res[0]\n",
    "            NEG_at_20_lxr += res[1]\n",
    "\n",
    "        last_pos_at_20 = np.mean(POS_at_20_lxr)/random_sampled_array.shape[0]\n",
    "        last_neg_at_20 = np.mean(NEG_at_20_lxr)/random_sampled_array.shape[0]\n",
    "        run_pos_at_20.append(last_pos_at_20)\n",
    "        run_neg_at_20.append(last_neg_at_20)\n",
    "        metric_for_monitoring.append(last_pos_at_20.item())\n",
    "\n",
    "        val_metrics = {\n",
    "            \"val/pos_at_20\": last_pos_at_20,\n",
    "            \"val/neg_at_20\": last_neg_at_20\n",
    "        }\n",
    "        \n",
    "        wandb.log({**train_metrics, **val_metrics})\n",
    "        print(f'Finished epoch {epoch} with run_pos_at_20 {last_pos_at_20} and run_neg_at_20 {last_neg_at_20}')\n",
    "        print(f'Train loss = {train_loss}')\n",
    "        if epoch>=5: # early stop conditions - if both pos@20 and neg@20 are getting worse in the past 4 epochs\n",
    "            if run_pos_at_20[-2]<run_pos_at_20[-1] and run_pos_at_20[-3]<run_pos_at_20[-2] and run_pos_at_20[-4]<run_pos_at_20[-3]:\n",
    "                if run_neg_at_20[-2]>run_neg_at_20[-1] and run_neg_at_20[-3]>run_neg_at_20[-2] and run_neg_at_20[-4]>run_neg_at_20[-3]:\n",
    "                    print(f'Early stop at trial with lambda_pos = {lambda_pos}, lambda_neg = {lambda_neg}, alpha_parameter = {alpha}. Best results at epoch {np.argmin(run_pos_at_20)} with value {np.min(run_pos_at_20)}')\n",
    "                    return np.min(metric_for_monitoring) # return the best pos@20 value in this trial\n",
    "\n",
    "    print(f'Stop at trial with lambda_pos = {lambda_pos}, lambda_neg = {lambda_neg}, alpha_parameter = {alpha}. Best results at epoch {np.argmin(run_pos_at_20)} with value {np.min(run_pos_at_20)}')    \n",
    "    return np.min(metric_for_monitoring) # return the best pos@20 value in this trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86931e41",
   "metadata": {},
   "source": [
    "### Save logs in text file, optimize using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fc6ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "logger.addHandler(logging.FileHandler(f\"{data_name}_{recommender_name}_explainer_training.log\", mode=\"w\"))\n",
    "\n",
    "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "wandb.init(project=f\"{data_name}_{recommender_name}_LXR_training\")\n",
    "logger.info(\"Start optimization.\")\n",
    "study.optimize(lxr_training, n_trials=10)\n",
    "\n",
    "with open(f\"{data_name}_{recommender_name}_explainer_training.log\") as f:\n",
    "    assert f.readline().startswith(\"A new study created\")\n",
    "    assert f.readline() == \"Start optimization.\\n\"\n",
    "    \n",
    "    \n",
    "# Print best hyperparameters and corresponding metric value\n",
    "print(\"Best hyperparameters: {}\".format(study.best_params))\n",
    "print(\"Best metric value: {}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bc5df-37cd-4cfd-bce6-b26fa795f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"ML1M_MLP_LXR_training\")\n",
    "trial_15 = [run for run in runs if \"trial_15\" in run.name][0]\n",
    "history = trial_15.history()\n",
    "\n",
    "# Print all epochs and their val/pos_at_20 scores\n",
    "print(\"Epoch\\tval/pos_at_20\\tval/neg_at_20\\tmetric\")\n",
    "for i, row in enumerate(history.iterrows()):\n",
    "    pos = row[1][\"val/pos_at_20\"]\n",
    "    neg = row[1][\"val/neg_at_20\"]\n",
    "    metric = neg - pos  # This is what we're maximizing based on the code\n",
    "    print(f\"{i}\\t{pos:.4f}\\t{neg:.4f}\\t{metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f375b3-f072-46ac-b5bb-a5f9b14fc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = max(range(len(history)), key=lambda i: history.iloc[i][\"val/neg_at_20\"] - history.iloc[i][\"val/pos_at_20\"])\n",
    "print(f\"\\nBest epoch based on metric (neg - pos): {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac1908-d5ec-4983-a3e6-b516875ea61b",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(study.best_trial.number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eeeb3e-fcbc-412c-9f6a-2a4f71afaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_name = \"ML1M\"\n",
    "recommender_name = \"NCF\"\n",
    "trial_num = 15\n",
    "best_epoch = 36\n",
    "explainer_hidden_size = 64\n",
    "lambda_pos = 21.029349760065738\n",
    "lambda_neg = 20.478401971323112\n",
    "\n",
    "lxr_path = f'LXR_{data_name}_{recommender_name}_{trial_num}_{best_epoch}_{explainer_hidden_size}_{lambda_pos}_{lambda_neg}.pt'\n",
    "checkpoint_path = Path(checkpoints_path, lxr_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint: {lxr_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f829f9a-2588-4155-8a3c-263c1f8c5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "checkpoints = glob.glob(f\"{checkpoints_path}/LXR_ML1M_MLP_1_*_64_32.37892294239854_43.97644946165088.pt\")\n",
    "print(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f9d5a-f9b7-4c97-92af-f9acc6529d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a690e95-70d6-406a-8b9d-12021d0c4c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
