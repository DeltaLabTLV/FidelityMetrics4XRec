{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9956ce",
   "metadata": {},
   "source": [
    "### This notebook presents the architectures of the three recommendation systems tested within this framework\n",
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6583e643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import optuna\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd66af",
   "metadata": {},
   "source": [
    "# 2. MLP recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb9ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, **kw):\n",
    "        super(MLP, self).__init__()\n",
    "        user_size = kw['num_items']\n",
    "        item_size = kw['num_items']\n",
    "        self.device = kw['device']\n",
    "        self.users_fc = nn.Linear(user_size, hidden_size, bias = True).to(self.device)\n",
    "        self.items_fc = nn.Linear(item_size, hidden_size, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_vec = self.users_fc(user_tensor.to(self.device))\n",
    "        item_vec = self.items_fc(item_tensor.to(self.device))\n",
    "        output = torch.matmul(user_vec, item_vec.T).to(self.device)\n",
    "        return self.sigmoid(output).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c694d",
   "metadata": {},
   "source": [
    "# 3. VAE recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65bb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, model_conf, **kw):\n",
    "        super().__init__()\n",
    "        self.device = kw['device']\n",
    "        num_features = kw['num_features'] \n",
    "        num_items = kw['num_items'] \n",
    "        self.demographic = kw['demographic'] \n",
    "        if self.demographic:\n",
    "            self.num_items = num_features\n",
    "            self.items_only = num_items\n",
    "        else:\n",
    "            self.num_items = num_items\n",
    "        self.enc_dims = [self.num_items] + model_conf['enc_dims']\n",
    "        self.dec_dims = self.enc_dims[::-1]\n",
    "        self.dims = self.enc_dims + self.dec_dims[1:]\n",
    "        self.dropout = model_conf['dropout']\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.total_anneal_steps = model_conf['total_anneal_steps']\n",
    "        self.anneal_cap = model_conf['anneal_cap']\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.anneal = 0.\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.enc_dims[:-1], self.enc_dims[1:])):\n",
    "            if i == len(self.enc_dims[:-1]) - 1:\n",
    "                d_out *= 2\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.enc_dims[:-1]) - 1:\n",
    "                self.encoder.append(nn.ReLU())\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dec_dims[:-1], self.dec_dims[1:])):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.dec_dims[:-1]) - 1:\n",
    "                self.decoder.append(nn.ReLU())\n",
    "                \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, rating_matrix, return_latent=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return latent variables\n",
    "        \n",
    "        Args:\n",
    "            rating_matrix: Input rating matrix\n",
    "            return_latent: If True, returns reconstruction, mean and logvar. If False, returns only reconstruction\n",
    "        \"\"\"\n",
    "        # Encoder forward pass\n",
    "        if len(rating_matrix.shape) == 1:\n",
    "            rating_matrix = torch.unsqueeze(rating_matrix, 0)\n",
    "        h = F.dropout(F.normalize(rating_matrix, dim=-1), p=self.dropout, training=self.training)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            h = layer(h)\n",
    "    \n",
    "        # Sample from latent space\n",
    "        mu_q = h[:, :self.enc_dims[-1]]\n",
    "        logvar_q = h[:, self.enc_dims[-1]:]\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        \n",
    "        epsilon = torch.zeros_like(std_q).normal_(mean=0, std=1.0)  # Changed std to 1.0\n",
    "        sampled_z = mu_q + self.training * epsilon * std_q\n",
    "        \n",
    "        output = sampled_z\n",
    "        for layer in self.decoder:\n",
    "            output = layer(output)\n",
    "            \n",
    "        if self.training:\n",
    "            kl_loss = ((0.5 * (-logvar_q + torch.exp(logvar_q) + torch.pow(mu_q, 2) - 1)).sum(1)).mean()\n",
    "            return self.softmax(output), kl_loss, mu_q, std_q  # Return consistent outputs\n",
    "        else:\n",
    "            if self.demographic:\n",
    "                return self.softmax(output[:,:self.items_only])\n",
    "            return self.softmax(output)\n",
    "        \n",
    "    def train_one_epoch(self, dataset, optimizer, batch_size, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Train model for one epoch\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        train_matrix = dataset\n",
    "        num_training = train_matrix.shape[0]\n",
    "        num_batches = int(np.ceil(num_training / batch_size))\n",
    "        perm = np.random.permutation(num_training)\n",
    "        loss = 0.0\n",
    "    \n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]\n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx]).to(self.device)\n",
    "    \n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.anneal_cap, 1. * self.update_count / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.anneal_cap\n",
    "    \n",
    "            # Get reconstructions, mean, and logvar from forward pass\n",
    "            pred_matrix, mu_q, logvar_q = self.forward(batch_matrix, return_latent=True)\n",
    "    \n",
    "            # Calculate losses\n",
    "            # Cross entropy loss\n",
    "            total_ce = -(F.log_softmax(pred_matrix, 1) * batch_matrix)\n",
    "            ce_hist = total_ce[:,:self.num_items].sum(1).mean()\n",
    "            ce_demo = total_ce[:,self.num_items:].sum(1).mean() if self.demographic else 0\n",
    "            ce_loss = ce_hist + alpha * ce_demo\n",
    "    \n",
    "            # KL divergence loss\n",
    "            kl_loss = ((0.5 * (-logvar_q + torch.exp(logvar_q) + torch.pow(mu_q, 2) - 1)).sum(1)).mean()\n",
    "    \n",
    "            # Total loss\n",
    "            batch_loss = ce_loss + kl_loss * self.anneal\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            self.update_count += 1\n",
    "            loss += batch_loss\n",
    "    \n",
    "            if b % 200 == 0:\n",
    "                print('(%3d / %3d) loss = %.4f' % (b, num_batches, batch_loss))\n",
    "    \n",
    "        return loss\n",
    "\n",
    "    def predict(self, eval_users, test_batch_size):\n",
    "        \"\"\"\n",
    "        Predict the model on test set\n",
    "        :param eval_users: evaluation (test) user\n",
    "        :param eval_pos: position of the evaluated (test) item\n",
    "        :param test_batch_size: batch size for test set\n",
    "        :return: predictions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            input_matrix = torch.Tensor(eval_users).to(self.device)\n",
    "            preds = np.zeros_like(input_matrix.cpu())\n",
    "\n",
    "            num_data = input_matrix.shape[0]\n",
    "            num_batches = int(np.ceil(num_data / test_batch_size))\n",
    "            perm = list(range(num_data))\n",
    "            for b in range(num_batches):\n",
    "                if (b + 1) * test_batch_size >= num_data:\n",
    "                    batch_idx = perm[b * test_batch_size:]\n",
    "                else:\n",
    "                    batch_idx = perm[b * test_batch_size: (b + 1) * test_batch_size]\n",
    "                    \n",
    "                test_batch_matrix = input_matrix[batch_idx]\n",
    "                batch_pred_matrix = self.forward(test_batch_matrix)\n",
    "                batch_pred_matrix = batch_pred_matrix.masked_fill(test_batch_matrix.bool(), float('-inf'))\n",
    "                preds[batch_idx] = batch_pred_matrix.detach().cpu().numpy()\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b4920-c062-47ac-ba14-0851f8386d5a",
   "metadata": {},
   "source": [
    "Enhanced VAE Recommender with Dynamic Epoch Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd280be-5384-4636-b490-646a57fa0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class EnhancedVAE(nn.Module):\n",
    "    def __init__(self, model_conf, **kw):\n",
    "        super(EnhancedVAE, self).__init__()\n",
    "        self.device = kw['device']\n",
    "        num_features = kw['num_features']\n",
    "        num_items = kw['num_items']\n",
    "        self.demographic = kw['demographic']\n",
    "        \n",
    "        if self.demographic:\n",
    "            self.num_items = num_features\n",
    "            self.items_only = num_items\n",
    "        else:\n",
    "            self.num_items = num_items\n",
    "            \n",
    "        self.enc_dims = [self.num_items] + model_conf['enc_dims']\n",
    "        self.dec_dims = self.enc_dims[::-1]\n",
    "        self.dims = self.enc_dims + self.dec_dims[1:]\n",
    "        self.dropout = model_conf['dropout']\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Training configuration\n",
    "        self.total_anneal_steps = model_conf['total_anneal_steps']\n",
    "        self.anneal_cap = model_conf['anneal_cap']\n",
    "        self.eps = 1e-6\n",
    "        self.anneal = 0.\n",
    "        self.update_count = 0\n",
    "        \n",
    "        # Early stopping configuration\n",
    "        self.patience = model_conf.get('patience', 5)\n",
    "        self.min_delta = model_conf.get('min_delta', 0.001)\n",
    "        self.performance_threshold = model_conf.get('performance_threshold', 0.20)\n",
    "        \n",
    "        # Initialize encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.enc_dims[:-1], self.enc_dims[1:])):\n",
    "            if i == len(self.enc_dims[:-1]) - 1:\n",
    "                d_out *= 2\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.enc_dims[:-1]) - 1:\n",
    "                self.encoder.append(nn.ReLU())\n",
    "\n",
    "        # Initialize decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dec_dims[:-1], self.dec_dims[1:])):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.dec_dims[:-1]) - 1:\n",
    "                self.decoder.append(nn.ReLU())\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, rating_matrix):\n",
    "        # Encoder forward pass\n",
    "        if len(rating_matrix.shape) == 1:\n",
    "            rating_matrix = torch.unsqueeze(rating_matrix, 0)\n",
    "        h = F.dropout(F.normalize(rating_matrix, dim=-1), p=self.dropout, training=self.training)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            h = layer(h)\n",
    "\n",
    "        # Sample from latent space\n",
    "        mu_q = h[:, :self.enc_dims[-1]]\n",
    "        logvar_q = h[:, self.enc_dims[-1]:]\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        \n",
    "        epsilon = torch.zeros_like(std_q).normal_(mean=0, std=0.01)\n",
    "        sampled_z = mu_q + self.training * epsilon * std_q\n",
    "\n",
    "        # Decoder forward pass\n",
    "        output = sampled_z\n",
    "        for layer in self.decoder:\n",
    "            output = layer(output)\n",
    "\n",
    "        if self.training:\n",
    "            kl_loss = ((0.5 * (-logvar_q + torch.exp(logvar_q) + torch.pow(mu_q, 2) - 1)).sum(1)).mean()\n",
    "            return output, kl_loss\n",
    "        else:\n",
    "            if self.demographic:\n",
    "                return self.softmax(output[:,:self.items_only])\n",
    "            else:\n",
    "                return self.softmax(output)\n",
    "\n",
    "    def train_with_dynamic_epochs(self, train_data, valid_data, optimizer, batch_size, max_epochs=100, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Train the model with dynamic epoch selection based on performance criteria\n",
    "        \"\"\"\n",
    "        best_metric = float('-inf')\n",
    "        patience_counter = 0\n",
    "        best_epoch = 0\n",
    "        training_history = []\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Train for one epoch\n",
    "            train_loss = self.train_one_epoch(train_data, optimizer, batch_size, alpha)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            hr10, hr50, hr100, mrr, mpr = self.evaluate(valid_data)\n",
    "            current_metric = hr10  # Using HR@10 as primary metric\n",
    "            \n",
    "            # Store training history\n",
    "            training_history.append({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'hr10': hr10,\n",
    "                'hr50': hr50,\n",
    "                'hr100': hr100,\n",
    "                'mrr': mrr,\n",
    "                'mpr': mpr\n",
    "            })\n",
    "            \n",
    "            # Check if performance meets threshold criteria\n",
    "            if current_metric > best_metric + self.min_delta:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = self.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping checks\n",
    "            if patience_counter >= self.patience:\n",
    "                break\n",
    "                \n",
    "            # Performance threshold check\n",
    "            if current_metric >= self.performance_threshold:\n",
    "                break\n",
    "                \n",
    "            print(f'Epoch {epoch}: HR@10 = {hr10:.4f}, Loss = {train_loss:.4f}')\n",
    "        \n",
    "        # Restore best model state\n",
    "        self.load_state_dict(best_model_state)\n",
    "        return best_epoch, training_history\n",
    "\n",
    "    def evaluate(self, eval_data, batch_size=128):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation/test data\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Implement evaluation metrics calculation\n",
    "            hr10, hr50, hr100, mrr, mpr = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            num_users = len(eval_data)\n",
    "            \n",
    "            for i in range(0, num_users, batch_size):\n",
    "                batch_users = eval_data[i:min(i + batch_size, num_users)]\n",
    "                batch_predictions = self.forward(torch.Tensor(batch_users).to(self.device))\n",
    "                \n",
    "                # Calculate metrics for batch\n",
    "                batch_hr10 = self.calculate_hit_ratio(batch_predictions, k=10)\n",
    "                batch_hr50 = self.calculate_hit_ratio(batch_predictions, k=50)\n",
    "                batch_hr100 = self.calculate_hit_ratio(batch_predictions, k=100)\n",
    "                batch_mrr = self.calculate_mrr(batch_predictions)\n",
    "                batch_mpr = self.calculate_mpr(batch_predictions)\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                hr10 += batch_hr10\n",
    "                hr50 += batch_hr50\n",
    "                hr100 += batch_hr100\n",
    "                mrr += batch_mrr\n",
    "                mpr += batch_mpr\n",
    "            \n",
    "            # Average metrics\n",
    "            hr10 /= num_users\n",
    "            hr50 /= num_users\n",
    "            hr100 /= num_users\n",
    "            mrr /= num_users\n",
    "            mpr /= num_users\n",
    "            \n",
    "        return hr10, hr50, hr100, mrr, mpr\n",
    "\n",
    "    def calculate_hit_ratio(self, predictions, k):\n",
    "        \"\"\"Calculate Hit Ratio @ k\"\"\"\n",
    "        _, top_k = torch.topk(predictions, k, dim=1)\n",
    "        return float(torch.any(top_k == self.target_items.unsqueeze(1), dim=1).float().mean())\n",
    "\n",
    "    def calculate_mrr(self, predictions):\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        ranks = torch.argmax(predictions == self.target_items.unsqueeze(1), dim=1).float() + 1\n",
    "        return float((1.0 / ranks).mean())\n",
    "\n",
    "    def calculate_mpr(self, predictions):\n",
    "        \"\"\"Calculate Mean Percentile Rank\"\"\"\n",
    "        ranks = torch.argmax(predictions == self.target_items.unsqueeze(1), dim=1).float() + 1\n",
    "        return float((ranks / predictions.size(1)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e79158",
   "metadata": {},
   "source": [
    "# 4. NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427f6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF_model(nn.Module):\n",
    "    def __init__(self, hidden_size=8, **kw):\n",
    "        super(GMF_model, self).__init__()\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        self.embed_user_GMF = nn.Linear(user_size, hidden_size, bias = False).to(self.device)\n",
    "        self.embed_item_GMF = nn.Linear(item_size, hidden_size, bias = False).to(self.device)\n",
    "        self.predict_layer = nn.Linear(hidden_size, 1, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_vec = self.embed_user_GMF(user_tensor.to(self.device))\n",
    "        item_vec = self.embed_item_GMF(item_tensor.to(self.device))\n",
    "        if user_vec.shape!=item_vec.shape:\n",
    "            user_res = torch.zeros(item_vec.shape).to(self.device)\n",
    "            user_res[:] = user_vec\n",
    "            user_vec = user_res\n",
    "            \n",
    "        output = self.predict_layer(torch.mul(user_vec, item_vec))\n",
    "        \n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7950c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, **kw):\n",
    "        super(MLP_model, self).__init__()\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        factor_num = hidden_size\n",
    "        self.embed_user_MLP = nn.Linear(user_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "        self.embed_item_MLP = nn.Linear(item_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "        \n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=0.5))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2).to(self.device))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(hidden_size, 1, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        embed_user_MLP = self.embed_user_MLP(user_tensor.to(self.device))\n",
    "        embed_item_MLP = self.embed_item_MLP(item_tensor.to(self.device))\n",
    "        if embed_user_MLP.shape!=embed_item_MLP.shape:\n",
    "            user_res = torch.zeros(embed_item_MLP.shape).to(self.device)\n",
    "            user_res[:] = embed_user_MLP\n",
    "            embed_user_MLP = user_res\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "        output = self.predict_layer(output_MLP)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525f036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, factor_num, num_layers,\n",
    "                    dropout, model, GMF_model=None, MLP_model=None, **kw):\n",
    "        super(NCF, self).__init__()\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        model: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
    "        GMF_model: pre-trained GMF weights;\n",
    "        MLP_model: pre-trained MLP weights.\n",
    "        \"\"\"        \n",
    "        self.dropout = dropout\n",
    "        self.model = model\n",
    "        self.GMF_model = GMF_model\n",
    "        self.MLP_model = MLP_model\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        self.embed_user_GMF = nn.Linear(user_size, factor_num, bias = False)\n",
    "        self.embed_item_GMF = nn.Linear(item_size, factor_num, bias = False)\n",
    "        self.embed_user_MLP = nn.Linear(\n",
    "                user_size, factor_num * (2 ** (num_layers - 1)), bias = False)\n",
    "        self.embed_item_MLP = nn.Linear(\n",
    "                item_size, factor_num * (2 ** (num_layers - 1)), bias = False)\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=self.dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "\n",
    "        if self.model in ['MLP', 'GMF']:\n",
    "            predict_size = factor_num \n",
    "        else:\n",
    "            predict_size = factor_num * 2\n",
    "        self.predict_layer = nn.Linear(predict_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self._init_weight_()\n",
    "        \n",
    "        # Move the entire model to the specified device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        \"\"\" We leave the weights initialization here. \"\"\"\n",
    "        if not self.model == 'NeuMF-pre':\n",
    "            nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "\n",
    "            for m in self.MLP_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.kaiming_uniform_(self.predict_layer.weight, \n",
    "                                    a=1, nonlinearity='sigmoid')\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        else:\n",
    "            # embedding layers\n",
    "            self.embed_user_GMF.weight.data.copy_(\n",
    "                            self.GMF_model.embed_user_GMF.weight)\n",
    "            self.embed_item_GMF.weight.data.copy_(\n",
    "                            self.GMF_model.embed_item_GMF.weight)\n",
    "            self.embed_user_MLP.weight.data.copy_(\n",
    "                            self.MLP_model.embed_user_MLP.weight)\n",
    "            self.embed_item_MLP.weight.data.copy_(\n",
    "                            self.MLP_model.embed_item_MLP.weight)\n",
    "\n",
    "            # mlp layers\n",
    "            for (m1, m2) in zip(\n",
    "                self.MLP_layers, self.MLP_model.MLP_layers):\n",
    "                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
    "                    m1.weight.data.copy_(m2.weight)\n",
    "                    m1.bias.data.copy_(m2.bias)\n",
    "\n",
    "            # predict layers\n",
    "            predict_weight = torch.cat([\n",
    "                self.GMF_model.predict_layer.weight, \n",
    "                self.MLP_model.predict_layer.weight], dim=1)\n",
    "            precit_bias = self.GMF_model.predict_layer.bias + \\\n",
    "                        self.MLP_model.predict_layer.bias\n",
    "\n",
    "            self.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
    "            self.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user = user.to(self.device)\n",
    "        item = item.to(self.device)\n",
    "        \n",
    "        if not self.model == 'MLP':\n",
    "            embed_user_GMF = self.embed_user_GMF(user)\n",
    "            embed_item_GMF = self.embed_item_GMF(item)\n",
    "            if embed_user_GMF.shape!=embed_item_GMF.shape:\n",
    "                user_res = torch.zeros(embed_item_GMF.shape, device=self.device)\n",
    "                user_res[:] = embed_user_GMF\n",
    "                embed_user_GMF = user_res\n",
    "            output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        if not self.model == 'GMF':\n",
    "            embed_user_MLP = self.embed_user_MLP(user)\n",
    "            embed_item_MLP = self.embed_item_MLP(item)\n",
    "            if embed_user_MLP.shape!=embed_item_MLP.shape:\n",
    "                user_res = torch.zeros(embed_item_MLP.shape, device=self.device)\n",
    "                user_res[:] = embed_user_MLP\n",
    "                embed_user_MLP = user_res\n",
    "            interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "            output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        if self.model == 'GMF':\n",
    "            concat = output_GMF\n",
    "        elif self.model == 'MLP':\n",
    "            concat = output_MLP\n",
    "        else:\n",
    "            concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        prediction = self.sigmoid(prediction)\n",
    "        return prediction.view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
